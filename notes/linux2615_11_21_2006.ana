11/20/2006, still not using inter-file pta, but did change "sameLval" 
comparison function for flagging warnings:

<= 1322 have locks out of 2384 warnings

out of a sample of 116 warnings:

diff lvals:

    only one locked:             60
    two-locked unifiable:         9
    two-locked not unifiable:    14
    total:                       83

same lvals:

    >= 1 locked total:           33

------------------------------------------------------------------
TODO: 

 - speed up symstate analysis by not havoc'ing unneeded lvals?
   (speeds up warning generation)

 - finish field-based (context-sensitive) or some other pta w/ field info


------------------------------------------------------------------
More Examples on how to show different lvals are really different:



============================================================
Possible race between access to:
        df_list.next : arch/i386/mm/pageattr.c:18 and
        (krxtimod_list.next)->next : net/rxrpc/krxtimod.c:26
        Accessed at locs:
        include/linux/list.h:313 and
        net/rxrpc/krxtimod.c:164
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {cpa_lock#tbd, socket->skt_sem#tbd, } (2)

        made empty at: :-1
        LS for 2nd access:
L+ = {krxtimod_lock#tbd, } (1)

        made empty at: :-1
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: net/rxrpc/krxiod.c:159 w/ func: rxrpc_krxiod




-------
first:

df_list is a static variable, not dynamically allocated

static struct list_head df_list = LIST_HEAD_INIT(df_list);


-------
second:

krxtimod_list is also a static variable, but what does its next field point to?

static LIST_HEAD(krxtimod_list);


void rxrpc_krxtimod_add_timer(rxrpc_timer_t *timer, unsigned long timeout)
{
	struct list_head *_p;
	rxrpc_timer_t *ptimer;


	spin_lock(&krxtimod_lock);

	list_del(&timer->link);

    ...

	list_for_each(_p, &krxtimod_list) {
		ptimer = list_entry(_p, rxrpc_timer_t, link);
        ...
	}

	list_add_tail(&timer->link, _p); /* insert before stopping point */

	spin_unlock(&krxtimod_lock);

	wake_up(&krxtimod_sleepq);


} /* end rxrpc_krxtimod_add_timer() */


it points to timer->link (which is a list node)...

above function is called by:


/*****************************************************************************/
/*
 * finish with a peer record
 * - it gets sent to the graveyard from where it can be resurrected or timed
 *   out
 */
void rxrpc_put_peer(struct rxrpc_peer *peer)
{

    ...

	rxrpc_krxtimod_add_timer(&peer->timeout, rxrpc_peer_timeout * HZ);

    ...

}    


which is called by... many, but looks like peers can come from:


/*****************************************************************************/
/*
 * find a peer record on the specified transport
 * - returns (if successful) with peer record usage incremented
 * - resurrects it from the graveyard if found there
 */
int rxrpc_peer_lookup(struct rxrpc_transport *trans, __be32 addr,
		      struct rxrpc_peer **_peer)
{
	struct rxrpc_peer *peer, *candidate = NULL;
	struct list_head *_p;
	int ret;

	/* [common case] search the transport's active list first */
	read_lock(&trans->peer_lock);
	list_for_each(_p, &trans->peer_active) {
		peer = list_entry(_p, struct rxrpc_peer, link);
		if (peer->addr.s_addr == addr)
			goto found_active;
	}
	read_unlock(&trans->peer_lock);

	/* [uncommon case] not active - create a candidate for a new record */
	ret = __rxrpc_create_peer(trans, addr, &candidate);  <<<< CREATE

    ...

	//sets peer = candidate, then adds peer to some active list, etc...

}

let's look at how the __rxrpc_create_peer works:

static int __rxrpc_create_peer(struct rxrpc_transport *trans, __be32 addr,
			       struct rxrpc_peer **_peer)
{
	struct rxrpc_peer *peer;

	/* allocate and initialise a peer record */
	peer = kmalloc(sizeof(struct rxrpc_peer), GFP_KERNEL);

    ...

	memset(peer, 0, sizeof(struct rxrpc_peer));

    ...

	rxrpc_timer_init(&peer->timeout, &rxrpc_peer_timer_ops);

    ...

	*_peer = peer;

	return 0;
}


timer_init does an INIT_LIST_HEAD(&timer->link), which:

#define INIT_LIST_HEAD(ptr) do { \
    (ptr)->next = (ptr); (ptr)->prev = (ptr); \
} while (0)


so &(peer->timeout.link)->next = &(peer->timeout.link), etc...

since (*peer) is fresh from the malloc, (*peer).timeout.link is fresh,
so &(peer->timeout.link)->next pts to a fresh obj for now...

where does it start adding timers?

rxrpc_krxtimod_add_timer(&peer->timeout, CONSTANT);

void rxrpc_krxtimod_add_timer(rxrpc_timer_t *timer, unsigned long timeout)
{
	struct list_head *_p;
	rxrpc_timer_t *ptimer;

    ...

	timer->timo_jif = jiffies + timeout;

	list_for_each(_p, &krxtimod_list) {
		ptimer = list_entry(_p, rxrpc_timer_t, link);
		if (time_before(timer->timo_jif, ptimer->timo_jif))
			break;
	}

	list_add_tail(&timer->link, _p); /* insert before stopping point */

    ...

}

so it mixes with the krxtimod_list, closing a flow loop?




============================================================
Possible race between access to:
        (pm_devs.next)->next : kernel/power/pm.c:44 and
        pdflush_list.next : mm/pdflush.c:47
        Accessed at locs:
        kernel/power/pm.c:253 and
        include/linux/list.h:223
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {pm_devs_lock#tbd, } (1)

        made empty at: :-1
        LS for 2nd access:
L+ = {pdflush_lock#tbd, } (1)

        made empty at: :-1
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: init/main.c:394 w/ func: init

...


should be similar as long as you track the lists separately



============================================================
Possible race between access to:
        df_list.next : arch/i386/mm/pageattr.c:18 and
        (dentry_unused.next)->next : fs/dcache.c:65
        Accessed at locs:
        include/linux/list.h:313 and
        fs/dcache.c:465
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {kafsasyncd_async_lock#tbd, } (1)

        made empty at: :-1
        LS for 2nd access:
L+ = {dcache_lock#tbd, (smb_servers.next)->sem#tbd, } (2)

        made empty at: :-1
        Th. 1 spawned: fs/afs/kafsasyncd.c:60 w/ func: kafsasyncd
        Th. 2 spawned: fs/smbfs/smbiod.c:76 w/ func: smbiod




--------
similar to previous report with df_list...

dentry_unused should only point to things added to list nodes added 
that list of list nodes, and df_list should never have been added to that list

as long as there's separate nodes for each struct list_head it's ok




============================================================
Possible race between access to:
(((_a137_408448_mempool->pages.next)->wb_context)->dentry)->d_inode : mm/mempool.c:115 and
((platform_bus.kobj.kset)->dentry)->d_inode : drivers/base/platform.c:25
        Accessed at locs:
        fs/nfs/write.c:1103 and
        fs/dcache.c:101
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;

        made empty at: fs/nfs/inode.c:1015
        LS for 2nd access:
L+ = {(bus_subsys.kset.dentry)->d_lock#tbd, (bus_subsys.kset.kobj.dentry)->d_lock#tbd, ((bus_subsys.kset.dentry)->d_inode)->i_sem#tbd, ((bus_subsys.kset.kobj.dentry)->d_inode)->i_sem#tbd, } (4)

        made empty at: :-1
        Th. 1 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread
        Th. 2 spawned: init/main.c:394 w/ func: init


----------
_a137_408448_mempool is from:

mempool_resize()... where they kmalloc a generic block of blocks of memory?

...

can do the field based analysis as long as:

struct nfs_open_context's (the type of wb_context) dentry field

is kept separate from:

struct kobject's (the type of platform_bus.kobj.kset (if ptr arith was fixed))
dentry field

...err... and their targets are kept separate (but the target is a struct
of the same type)...


-------------------------------------------------------
looking at where dentry is set in /nfs (first access):

d_alloc, d_instantiate, d_lookup, nfs_readdir_lookup, filp->f_dentry, some other sources from nfs4proc and dir.c


d_alloc: does a kmem_cache_alloc(dentry_cache), inits, then returns...
at least it looks like the cache is only used for dentries 

(where do we cut it off and say that the func is like malloc ? __alloc_pages()? cache_grow()? kmem_cache_alloc itself ?)



d_instantiate: hooks the dentry & an inode up?

d_lookup: searches through a hashtable for subdir given parentdir + name of sub

nfs_readdir_lookup: do d_lookup -- if not found, do d_alloc

filp->f_dentry (when it gets set...):


------------------------------------------------------

There may be way too many places that a dentry flows (hashtable), and the
source is a cache, so it's hard to show that the lvals are the different, but
it may be easier to show that the threads don't interact?


------------------------------------------------------
First access:

static void nfs_writeback_done_full(struct nfs_write_data *data, int status)
{
	struct nfs_page		*req;
	struct page		*page;

	/* Update attributes as result of writeback. */
	while (!list_empty(&data->pages)) {
		req = nfs_list_entry(data->pages.next);
		nfs_list_remove_request(req);
		page = req->wb_page;

		dprintk("NFS: write (%s/%Ld %d@%Ld)",
			req->wb_context->dentry->d_inode->i_sb->s_id,
			(long long)NFS_FILEID(req->wb_context->dentry->d_inode),
			req->wb_bytes,
			(long long)req_offset(req));

        ^^^ READS
...

}

Read for logging... probably ok

------------------------------------------------------
Second access:

/*
 * Release the dentry's inode, using the filesystem
 * d_iput() operation if defined.
 * Called with dcache_lock and per dentry lock held, drops both.
 */
static inline void dentry_iput(struct dentry * dentry)
{
	struct inode *inode = dentry->d_inode;
	if (inode) {
		dentry->d_inode = NULL;
		list_del_init(&dentry->d_alias);
		spin_unlock(&dentry->d_lock);
		spin_unlock(&dcache_lock);
		if (!inode->i_nlink)
			fsnotify_inoderemove(inode);
		if (dentry->d_op && dentry->d_op->d_iput)
			dentry->d_op->d_iput(dentry, inode);
		else
			iput(inode);
	} else {
		spin_unlock(&dentry->d_lock);
		spin_unlock(&dcache_lock);
	}
}

-------------------------------------------------------

ok... probably can't show that the threads don't interact, but


REASON: racey access is a read for logging



============================================================
Possible race between access to:
        ((c->gc_task)->sighand)->action[0].sa.sa_handler : fs/jffs2/background.c:33 an
d
        ((init_task.tasks.next)->sighand)->action[0].sa.sa_handler : include/linux/sch
ed.h:999
        Accessed at locs:
        kernel/signal.c:176 and
        kernel/signal.c:874
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {tasklist_lock#tbd, c->erase_completion_lock#tbd, ((c->gc_task)->sighand)->siglock#tbd, } (3)

        made empty at: :-1
        LS for 2nd access:
L+ = empty;

        made empty at: mm/oom_kill.c:286
        Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
        Th. 2 spawned: init/main.c:394 w/ func: init



see if 

(c->gc_task->sighand) point to the same thing as 
(init_task.tasks.next->sighand)

type-based test won't work because they are both sighands

--------------------
c->gc_task = current (current thread)



--------------------
init_task.tasks.next (random task in list of all schedulable tasks...)


indistinguishable, but are the accesses benign?


------------------
FIRST:

static int sig_ignored(struct task_struct *t, int sig)
{
	void __user * handler;

    ...

	/* Is it explicitly or implicitly ignored? */
	handler = t->sighand->action[sig-1].sa.sa_handler;       <<<< READ
	return   handler == SIG_IGN ||
		(handler == SIG_DFL && sig_kernel_ignore(sig));
}

callers assert that the lock is held

------------------
SECOND: 


/*
 * Force a signal that the process can't ignore: if necessary
 * we unblock the signal and change any SIG_IGN to SIG_DFL.
 */

int
force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
{
	unsigned long int flags;
	int ret;

	spin_lock_irqsave(&t->sighand->siglock, flags);
	if (t->sighand->action[sig-1].sa.sa_handler == SIG_IGN) {
		t->sighand->action[sig-1].sa.sa_handler = SIG_DFL;     <<< HERE
	}
	if (sigismember(&t->blocked, sig)) {
		sigdelset(&t->blocked, sig);
	}
	recalc_sigpending_tsk(t);
	ret = specific_send_sig_info(sig, info, t);
	spin_unlock_irqrestore(&t->sighand->siglock, flags);

	return ret;
}

looks like the lock is taken... where was the LS made empty? 
made empty at: mm/oom_kill.c:286

... out_of_memory calls a bunch of things, which call force_sig(SIG_KILL, ...)
and that just calls force_sig_info



REASON: diff lvals, but lost a lock somewhere?


============================================================
Possible race between access to:
        (mq->queue)->nr_sorted : drivers/mmc/mmc_queue.c:126 and
        ((_a793_449552_ll_rw_blk.bd_disk)->queue)->nr_sorted : block/ll_rw_blk.c:775
        Accessed at locs:
        block/elevator.c:385 and
        block/elevator.c:286
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {mq->thread_sem#tbd, } (1)

        made empty at: :-1
        LS for 2nd access:
L+ = empty;

        made empty at: block/ll_rw_blk.c:2772
        Th. 1 spawned: drivers/mmc/mmc_queue.c:161 w/ func: mmc_queue_thread
        Th. 2 spawned: drivers/net/irda/sir_kthread.c:482 w/ func: irda_thread


-------------
see how the queue pointers are set up for the mq


mmc_init_queue( mq, ...) {

    mq->queue = blk_init_queue(...)

}

which gets one from kmem_cache_alloc

------------
how does the bd_disk get its queue?

one possibility:

int swap_readpage(struct file *file, struct page *page)
{
	struct bio *bio;
	int ret = 0;

	BUG_ON(!PageLocked(page));
	ClearPageUptodate(page);

	bio = get_swap_bio(GFP_KERNEL, page_private(page), page, <<< HERE
				end_swap_bio_read);

	if (bio == NULL) {
		unlock_page(page);
		ret = -ENOMEM;
		goto out;
	}
	inc_page_state(pswpin);
	submit_bio(READ, bio);
out:
	return ret;
}

which eventually calls the 2nd access

static struct bio *get_swap_bio(gfp_t gfp_flags, pgoff_t index,
				struct page *page, bio_end_io_t end_io)
{
	struct bio *bio;

	bio = bio_alloc(gfp_flags, 1);
	if (bio) {
		struct swap_info_struct *sis;
		swp_entry_t entry = { .val = index, };

		sis = get_swap_info_struct(swp_type(entry));
		bio->bi_sector = map_swap_page(sis, swp_offset(entry)) *
					(PAGE_SIZE >> 9);
		bio->bi_bdev = sis->bdev;

        ...
	}
	return bio;
}


struct swap_info_struct *
get_swap_info_struct(unsigned type)
{
	return &swap_info[type];
}

struct swap_info_struct swap_info[MAX_SWAPFILES];

...

no idea where that guy gets its bdev ( and the bd_disk w/ the queue )



============================================================
Possible race between access to:
        kioctx_cachep->array[0] : fs/aio.c:51 and
        cache_cache.array[0] : mm/slab.c:620
        Accessed at locs:
        mm/slab.c:665 and
        mm/slab.c:1779
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {epsem#tbd, } (1)

        made empty at: :-1
        LS for 2nd access:
L+ = empty;

        made empty at: :-1
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: init/main.c:394 w/ func: init


see if kioctx_cachep ever points to cache_cache


static int __init aio_setup(void)
{
    ...

	kioctx_cachep = kmem_cache_create("kioctx", sizeof(struct kioctx),
				0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);

    ^^^ fresh!
    ...

	return 0;
}

so it doesn't



============================================================
Possible race between access to:
        (event_cachep->array[0])->avail : fs/inotify.c:43 and
        ((posix_timers_cache->nodelists[0])->shared)->avail : kernel/posix-timers.c:87
        Accessed at locs:
        mm/slab.c:2534 and
        mm/slab.c:2706
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;

        made empty at: mm/slab.c:2427
        LS for 2nd access:
L+ = empty;

        made empty at: kernel/fork.c:107
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread

(1)
        LS for 1st access:
L+ = empty;

        made empty at: mm/slab.c:2427
        LS for 2nd access:
L+ = empty;

        made empty at: kernel/fork.c:363
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread

(2)
        LS for 1st access:
L+ = empty;

        made empty at: mm/slab.c:2427
        LS for 2nd access:
L+ = {khpsbpkt_sig#tbd, (posix_timers_cache->nodelists[0])->list_lock#tbd, } (2)

        made empty at: :-1
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread



--------------------------------------
does (event_cachep->array[0]) alias with
((posix_timers_cache->nodelists[0])->shared) ?

---

bookmark