11/13/2006 (kernel 2.6.15) -- after updating code to handle infinite loops
inter-file PTA not used yet


2041 warnings, 566 have L+ non-empty...


TODO: need to improve fp/callgraph to check more parallel accesses.
need to determine if roots of callgraph should also be treated as threads
(missing the accesses from the bug reports!)


TRY: group by lval names (in addition to) access locations (maybe cuts warnings in half?)



===================================================

Possible race between access to:
        (c->erase_complete_list.next)->first_node : fs/jffs2/background.c:33 and
        (c->nextblock)->first_node : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/erase.c:376 and
        fs/jffs2/nodemgmt.c:364
        Possible paths & LS (first 3):


(0)
        LS for 1st access:
L+ = {c->erase_free_sem#tbd, } (1)
        made empty at: :-1
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/nodemgmt.c:162
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread



---
/* This must only ever be called when no GC thread is currently running */
int jffs2_start_garbage_collect_thread(struct jffs2_sb_info *c)
{

	if (c->gc_task)
		BUG();

	init_completion(&c->gc_thread_start);
	init_completion(&c->gc_thread_exit);

	pid = kernel_thread(jffs2_garbage_collect_thread, c, CLONE_FS|CLONE_FILES);

    ...

}


the base c object is that argument ^^^

hmm... must only be called when there's no other GC thread? Even checks for it?

lvals don't seem to match well either... probably different objects?
one is an object held in a list, the other is just kept around as a field?


------------
1st access :

static void jffs2_mark_erased_block(struct jffs2_sb_info *c, struct jffs2_eraseblock *jeb)
{
    ...

		jeb->first_node = jeb->last_node = NULL;  <<<< HERE

    ...
}


called from 

void jffs2_erase_pending_blocks(struct jffs2_sb_info *c, int count)
{
        
    ...
    down(&c->erase_free_sem);

	spin_lock(&c->erase_completion_lock);
	while (!list_empty(&c->erase_complete_list) ||
	       !list_empty(&c->erase_pending_list)) {

		if (!list_empty(&c->erase_complete_list)) {
			jeb = list_entry(c->erase_complete_list.next, struct jffs2_eraseblock, list);
			list_del(&jeb->list);
			spin_unlock(&c->erase_completion_lock);

			jffs2_mark_erased_block(c, jeb);  <<<< HERE

            ...
        } else { // go through the erase_pending_list instead
            ...
        }

		/* Be nice */
		cond_resched();
		spin_lock(&c->erase_completion_lock);
	}

	spin_unlock(&c->erase_completion_lock);
 done:
	D1(printk(KERN_DEBUG "jffs2_erase_pending_blocks completed\n"));

	up(&c->erase_free_sem);

}

...


-------------
2nd


/* Called with alloc sem _and_ erase_completion_lock */
static int jffs2_do_reserve_space(struct jffs2_sb_info *c, uint32_t minsize, uint32_t *ofs, uint32_t *len, uint32_t sumsize)
{
	struct jffs2_eraseblock *jeb = c->nextblock;
	
    ...

	if (c->cleanmarker_size && jeb->used_size == c->cleanmarker_size &&
	    !jeb->first_node->next_in_ino) {
		/* Only node in it beforehand was a CLEANMARKER node (we think).
		   So mark it obsolete now that there's going to be another node
		   in the block. This will reduce used_size to zero but We've
		   already set c->nextblock so that jffs2_mark_node_obsolete()
		   won't try to refile it to the dirty_list.
		*/
		spin_unlock(&c->erase_completion_lock);

		jffs2_mark_node_obsolete(c, jeb->first_node);  <<<<< HERE

		spin_lock(&c->erase_completion_lock);
	}

	D1(printk(KERN_DEBUG "jffs2_do_reserve_space(): Giving 0x%x bytes at 0x%x\n", *len, *ofs));
	return 0;
}


REASON: not sure if the two objs are the same (TODO try to prove they aren't)
otherwise, the locksets do look disjoint... also, ptr arith handling 
couldn't get the right lval name for container of list.next

What if the objs are the same... any negative consequences?



===========================
Where the objs are created:


~~~~~~~
lval 1: (c->erase_complete_list.next)->first_node

---
c:


int jffs2_do_fill_super(struct super_block *sb, void *data, int silent)
{
	struct jffs2_sb_info *c;
    ...
	c = JFFS2_SB_INFO(sb);
    ...
    c->inocache_list = kmalloc(INOCACHE_HASHSIZE * sizeof(struct jffs2_inode_cache *), GFP_KERNEL);
    ...
	if ((ret = jffs2_do_mount_fs(c)))
		goto out_inohash;
    ...
    // etc.
    ...
}

#define JFFS2_SB_INFO(sb) (sb->s_fs_info)


static struct super_block *jffs2_get_sb_mtd(struct file_system_type *fs_type,
					      int flags, const char *dev_name,
					      void *data, struct mtd_info *mtd)
{
	struct super_block *sb;
    struct jffs2_sb_info *c;
	c = kmalloc(sizeof(*c), GFP_KERNEL);
	sb = sget(fs_type, jffs2_sb_compare, jffs2_sb_set, c);

    // ^^^^ sets the s_fs_info field of sb to c, etc.

    ...

	init_MUTEX(&c->alloc_sem);
	init_MUTEX(&c->erase_free_sem);
	init_waitqueue_head(&c->erase_wait);
	init_waitqueue_head(&c->inocache_wq);
	spin_lock_init(&c->erase_completion_lock);
	spin_lock_init(&c->inocache_lock);

	sb->s_op = &jffs2_super_operations;
	sb->s_flags = flags | MS_NOATIME;

	ret = jffs2_do_fill_super(sb, data, (flags&MS_VERBOSE)?1:0);

    ...
}

-----------------------
c->erase_complete_list:

int jffs2_do_mount_fs(struct jffs2_sb_info *c)
{
    ...
    INIT_LIST_HEAD(&c->erase_complete_list);
    ...
}

#define INIT_LIST_HEAD(ptr) do { \
	(ptr)->next = (ptr); (ptr)->prev = (ptr); \
} while (0)

the erase_complete_list is allocated as a struct embedded in the
c struct (so the next/prev pointers are already there from the alloc for c)


when do objs get added to the list though?

static void jffs2_erase_succeeded(struct jffs2_sb_info *c, 
                                  struct jffs2_eraseblock *jeb)
{
	spin_lock(&c->erase_completion_lock);
	list_del(&jeb->list);
	list_add_tail(&jeb->list, &c->erase_complete_list);
	spin_unlock(&c->erase_completion_lock);
	/* Ensure that kupdated calls us again to mark them clean */
	jffs2_erase_pending_trigger(c);
}


static void jffs2_erase_callback(struct erase_info *instr)
{
	struct erase_priv_struct *priv = (void *)instr->priv;

    if (*) {
        ...
    } else {
   		jffs2_erase_succeeded(priv->c, priv->jeb);
    }
    kfree(instr);
}


static void jffs2_erase_block(struct jffs2_sb_info *c,
			      struct jffs2_eraseblock *jeb)
{
	struct erase_info *instr;
	instr = kmalloc(sizeof(struct erase_info) + sizeof(struct erase_priv_struct), GFP_KERNEL);
	memset(instr, 0, sizeof(*instr));

	instr->mtd = c->mtd;
	instr->addr = jeb->offset;
	instr->len = c->sector_size;
	instr->callback = jffs2_erase_callback;
	instr->priv = (unsigned long)(&instr[1]);
	instr->fail_addr = 0xffffffff;

	((struct erase_priv_struct *)instr->priv)->jeb = jeb;
	((struct erase_priv_struct *)instr->priv)->c = c;

    ret = c->mtd->erase(c->mtd, instr);

    ...

}

void jffs2_erase_pending_blocks(struct jffs2_sb_info *c, int count)
{
    ...


    	} else if (!list_empty(&c->erase_pending_list)) {
			jeb = list_entry(c->erase_pending_list.next, struct jffs2_eraseblock, list);

			list_del(&jeb->list);
			c->erasing_size += c->sector_size;
			c->wasted_size -= jeb->wasted_size;
            ...

			jffs2_erase_block(c, jeb);

        }

}


basically, erase_complete_list gets blocks from pending list on this path...
(how do entries enter the pending list?)

and

static void jffs2_mark_erased_block(struct jffs2_sb_info *c, 
                                    struct jffs2_eraseblock *jeb)
{

...

refile:
	/* Stick it back on the list from whence it came and come back later */
	jffs2_erase_pending_trigger(c);
	spin_lock(&c->erase_completion_lock);
	list_add(&jeb->list, &c->erase_complete_list);
	spin_unlock(&c->erase_completion_lock);

}



--------------------

~~~~~~~
lval 2:

c->nextblock

when does that get set?

in 

scan.c      from an c->blocks[i]
nodemgmt.c  from a c->free_list


-------------------

So does pending_list ever mix with blocks or free_list?



to be continued...



===================================================
Possible race between access to:
        ((socket->dev.kobj.kset)->dentry)->d_inode : drivers/pcmcia/cs.c:178 and
        ((socket->dev.dev)->kobj.dentry)->d_inode : drivers/pcmcia/cs.c:178
        Accessed at locs:
        fs/dcache.c:101 and
        fs/sysfs/inode.c:231
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(socket->dev.kobj.dentry)->d_lock#tbd, ((socket->dev.kobj.dentry)->d_inode)->i
_sem#tbd, } (2)
        made empty at: :-1
        LS for 2nd access:
L+ = empty;
        made empty at: fs/sysfs/inode.c:247
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd


(1)
        LS for 1st access:
L+ = {((socket->dev.dev)->kobj.dentry)->d_lock#tbd, (((socket->dev.dev)->kobj.dentry
)->d_inode)->i_sem#tbd, } (2)
        made empty at: :-1
        LS for 2nd access:
L+ = empty;
        made empty at: fs/sysfs/inode.c:247
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(2)
        LS for 1st access:
L+ = {(socket->dev.kobj.dentry)->d_lock#tbd, ((socket->dev.kobj.dentry)->d_inode)->i
_sem#tbd, ((socket->dev.kobj.kset)->dentry)->d_lock#tbd, (((socket->dev.kobj.kset)->
dentry)->d_inode)->i_sem#tbd, } (4)
        made empty at: :-1
        LS for 2nd access:
L+ = empty;
        made empty at: fs/sysfs/inode.c:247
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd


...


REASON: Different lvals (kset doesn't have dentry field... ptr arith
couldn't get the right lval name for a container_of(x) calculation?)


------------------
Attempt to prove 
*((socket->dev.kobj.kset)->dentry) != *(socket->dev.dev->kobj.dentry)


where does socket->dev.kobj get set, and socket->dev.dev get set?

hmmm.... kset doesn't have a dentry field! (but kobj itself does)

~~~~~~~
socket:

int pcmcia_register_socket(struct pcmcia_socket *socket)
{
    ... this is the socket that's the base!

	if (!socket || !socket->ops || !socket->dev.dev || !socket->resource_ops)
		return -EINVAL;

    ... so socket->dev.dev is created beforehand

    ret = kernel_thread(pccardd, socket, CLONE_KERNEL);
       if (ret < 0)
          goto err;
 
    wait_for_completion(&socket->thread_done);
}

static int pccardd(void *__skt)
{
    struct pcmcia_socket *skt = __skt;
    DECLARE_WAITQUEUE(wait, current);
    int ret;

    daemonize("pccardd");

    ...
    /* register with the device core */
    ret = class_device_register(&skt->dev);
    ...

} 


~~~~~~~~~~~~~~~~~
*.kobj:



sysfs_create_bin_file(&class_dev->kobj, &pccard_cis_attr)

int sysfs_create_bin_file(struct kobject * kobj, struct bin_attribute * attr)
{
	BUG_ON(!kobj || !kobj->dentry || !attr);

	return sysfs_add_file(kobj->dentry, &attr->attr, SYSFS_KOBJ_BIN_ATTR);
}

bleh... give up for now...



===================================================


Possible race between access to:
        ((socket->dev.kobj.kset)->dentry)->d_lru.prev : drivers/pcmcia/cs.c:178 and
        socket->dev.node.prev : drivers/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/list.h:165 and
        drivers/base/class.c:475
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(socket->dev.kobj.dentry)->d_lock#tbd, ((socket->dev.kobj.dentry)->d_inode)->i
_sem#tbd, } (2)

...



REASON: Different lvals (requires to point to an odd offset of another field)



===================================================

Possible race between access to:
((kafsasyncd_async_attnq.next)->call)->app_err_state : fs/afs/kafsasyncd.c:40 
and
((kafsasyncd_async_attnq.next)->call)->app_err_state : fs/afs/kafsasyncd.c:40
        Accessed at locs:
        net/rxrpc/call.c:863 and
        net/rxrpc/call.c:101
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: net/rxrpc/call.c:1266
        LS for 2nd access:
L+ = empty;
        made empty at: net/rxrpc/call.c:1016
        Th. 1 spawned: net/rxrpc/krxiod.c:159 w/ func: rxrpc_krxiod
        Th. 2 spawned: net/rxrpc/krxiod.c:159 w/ func: rxrpc_krxiod

(1)
        LS for 1st access:
L+ = {((kafsasyncd_async_attnq.next)->call)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: fs/afs/kafsasyncd.c:60 w/ func: kafsasyncd
        Th. 2 spawned: fs/afs/kafsasyncd.c:60 w/ func: kafsasyncd

-------
First:

static int __rxrpc_call_abort(struct rxrpc_call *call, int errno)
{
	struct rxrpc_connection *conn = call->conn;
	struct rxrpc_message *msg;
	struct kvec diov[1];
	int ret;
	__be32 _error;

	_enter("%p{%08x},%p{%d},%d",
	       conn, ntohl(conn->conn_id), call, ntohl(call->call_id), errno);

	/* if this call is already aborted, then just wake up any waiters */
	if (call->app_call_state == RXRPC_CSTATE_ERROR) {
		spin_unlock(&call->lock);
		call->app_error_func(call);
		_leave(" = 0");
		return 0;
	}

	rxrpc_get_call(call);

	/* change the state _with_ the lock still held */
	call->app_call_state	= RXRPC_CSTATE_ERROR;

	call->app_err_state	= RXRPC_ESTATE_LOCAL_ABORT;           <<<<< HERE

    ...

	_state(call);

	/* ask the app to translate the error code */
	call->app_aemap_func(call);

	spin_unlock(&call->lock);

    ...
	return ret;
} /* end __rxrpc_call_abort() */


holds some call->lock on entry, which is true for the second thread spawn
to the first access, but the first thread spawn?


        made empty at: net/rxrpc/call.c:1266

static void rxrpc_call_receive_data_packet(struct rxrpc_call *call,
					   struct rxrpc_message *msg)
{
    ...

	switch (ret) {
	case 0:
		spin_unlock(&call->lock);
		call->app_attn_func(call);
		break;
	case -EAGAIN:
		spin_unlock(&call->lock);
		break;
	case -ECONNABORTED:
		spin_unlock(&call->lock);
		break;
	default:

		__rxrpc_call_abort(call, ret);   <<<< Called first access func, should still hold the lock though... only unlocked on other paths?
		break;


	}

	_state(call);

	_leave("");

}

anyway, the first access should hold the lock always for the first thread
spawn and the other thread spawn never reaches the second access  



-------
Second:

static void rxrpc_call_default_aemap_func(struct rxrpc_call *call)
{
	switch (call->app_err_state) {     <<<< READ here
	case RXRPC_ESTATE_LOCAL_ABORT:
		call->app_abort_code = -call->app_errno;
	case RXRPC_ESTATE_PEER_ABORT:
		call->app_errno = -ECONNABORTED;
	default:
		break;
	}
}

Looks like most callers hold the call->lock, but why do we get:

        made empty at: net/rxrpc/call.c:1016

        ...

			spin_lock(&call->lock);
			call->app_call_state	= RXRPC_CSTATE_ERROR;

			call->app_err_state	= RXRPC_ESTATE_PEER_ABORT;     <<< HERE

			call->app_abort_code	= (dp ? ntohl(*dp) : 0);
			call->app_errno		= -ECONNABORTED;
			call->app_mark		= RXRPC_APP_MARK_EOF;
			call->app_read_buf	= NULL;
			call->app_async_read	= 0;

			/* ask the app to translate the error code */
			call->app_aemap_func(call);                        <<< HERE


			_state(call);
			spin_unlock(&call->lock);

        ...

lock is actually held... but my call graph thinks other functions
called this guy.


REASON: bogus FuncPointer-based calls (field insensitivity kicks in?)

===================================================
Possible race between access to:
  ((socket->dev.kobj.kset)->dentry)->d_lru.prev : drivers/pcmcia/cs.c:178
  (socket->dev.kobj.kset)->entry.prev : drivers/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/list.h:165 and
        include/linux/list.h:222
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(socket->dev.kobj.dentry)->d_lock#tbd, ((socket->dev.kobj.dentry)->d_inode)->i
_sem#tbd, ((socket->dev.kobj.kset)->dentry)->d_lock#tbd, (((socket->dev.kobj.kset)->
dentry)->d_inode)->i_sem#tbd, } (4)
        made empty at: :-1
        LS for 2nd access:
L+ = {(socket->dev.kobj.kset)->list_lock#tbd, } (1)

...

REASON: Different lvals + different locks (kset doesn't have dentry field)

---------------
Proof ?

not sure if it can be proved w/ alias analysis unless ptr arith is modelled
precisely (thanks to the kobj, kset fields) ...


unlikely that x.dentry points to (x.entry - bits_offset(d_lru)) though,
so we can use that?




===================================================
Possible race between access to:
        (c->blocks)->last_node : fs/jffs2/background.c:33 and
        (c->erase_complete_list.next)->last_node : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/nodemgmt.c:410 and
        fs/jffs2/erase.c:376
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: fs/jffs2/nodemgmt.c:733
        LS for 2nd access:
L+ = {c->erase_free_sem#tbd, } (1)
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


REASON: Different lvals, so skip for now

------------------------------------------------
Proof (see earlier attempt to use malloc sites)

need to fix ptr arith too... so that it doesn't say 
c->erase_complete_list.next, but some sort of thing like (c->blocks),
but what should the name be?


jeb = list_entry(c->erase_complete_list.next, struct jffs2_eraseblock, list);

#define list_entry(ptr, type, member) \
	container_of(ptr, type, member)


#define container_of(ptr, type, member) \
    ({ const typeof( ((type *)0)->member ) *__mptr = (ptr); \
       (type *)( (char *)__mptr - offsetof(type,member) );} \
    )




===================================================
Possible race between access to:
        suspends_pending : arch/i386/kernel/apm.c:390 and
        apm_info.connection_version : include/linux/apm_bios.h:109
        Accessed at locs:
        arch/i386/kernel/apm.c:1147 and
        arch/i386/kernel/apm.c:1740
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {user_list_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm



REASON: Different lvals (definite from memory layout)


static int			suspends_pending; // apm.c

struct apm_info	apm_info; //arch/i386/kernel/setup.c





===================================================
Possible race between access to:
        (rxrpc_krxiod_callq.next)->dsize : net/rxrpc/krxiod.c:30 and
        (rxrpc_krxiod_callq.next)->dsize : net/rxrpc/krxiod.c:30
        Accessed at locs:
        net/rxrpc/call.c:1755 and
        net/rxrpc/call.c:2219
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(rxrpc_krxiod_callq.next)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: net/rxrpc/krxiod.c:159 w/ func: rxrpc_krxiod
        Th. 2 spawned: net/rxrpc/krxiod.c:159 w/ func: rxrpc_krxiod



-----------
First :


/*****************************************************************************/
/*
 * transfer data from the ready packet queue to the asynchronous read buffer
 * - since this func is the only one going to look at packets queued on
 *   app_readyq, we don't need a lock to modify or access them, only to modify
 *   the queue pointers
 * - called with call->lock held
 * - the buffer must be in kernel space
 * - returns:
 *	0 if buffer filled
 *	-EAGAIN if buffer not filled and more data to come
 *	-EBADMSG if last packet received and insufficient data left
 *	-ECONNABORTED if the call has in an error state
 */
static int __rxrpc_call_read_data(struct rxrpc_call *call)
{

    ...

	while (!list_empty(&call->app_readyq) && call->app_mark > 0) {
		msg = list_entry(call->app_readyq.next,
				 struct rxrpc_message, link);

		/* drag as much data as we need out of this packet */
		qty = min(call->app_mark, msg->dsize);

		_debug("reading %Zu from skb=%p off=%lu",
		       qty, msg->pkt, msg->offset);

		if (call->app_read_buf)
			if (skb_copy_bits(msg->pkt, msg->offset,
					  call->app_read_buf, qty) < 0)
				panic("%s: Failed to copy data from packet:"
				      " (%p,%p,%Zd)",
				      __FUNCTION__,
				      call, call->app_read_buf, qty);

		/* if that packet is now empty, discard it */
		call->app_ready_qty -= qty;
		msg->dsize -= qty;                   <<<< HERE

		if (msg->dsize == 0) {
			list_del_init(&msg->link);
			rxrpc_put_message(msg);
		}
		else {
			msg->offset += qty;
		}

		call->app_mark -= qty;
		if (call->app_read_buf)
			call->app_read_buf += qty;
	}

    ...
}

Lock for the list entry isn't the thing that's held? It's just the top level
call->lock that should be held (according to function comment)



--------
Second :


/*****************************************************************************/
/*
 * resend NAK'd or unacknowledged packets up to the highest one specified
 */
static void rxrpc_call_resend(struct rxrpc_call *call, rxrpc_seq_t highest)
{
	struct rxrpc_message *msg;
	struct list_head *_p;
	rxrpc_seq_t seq = 0;

    ...

	found_msg:
		if (msg->state != RXRPC_MSG_SENT)
			continue; /* only un-ACK'd packets */

		rxrpc_get_message(msg);
		spin_unlock(&call->lock);


		/* send each message again (and ignore any errors we might
		 * incur) */
		_proto("Resending DATA message { ds=%Zu dc=%u df=%02lu }",
		       msg->dsize, msg->dcount, msg->dfree);  <<<< HERE!!


		if (rxrpc_conn_sendmsg(call->conn, msg) == 0)
			call->pkt_snd_count++;

		rxrpc_put_message(msg);

		spin_lock(&call->lock);
	}


}


#define kproto(FMT, a...)       printk("### "FMT"\n" , ##a)
#define _proto(FMT, a...)       kproto(FMT , ##a)

Looks like the read is just a printk thing... "rxrpc_conn_sendmsg" holds
the lock when it looks at/modifies msg->dsize

REASON: logging use (not too important, and maybe can't hold lock in case
console I/O blocks)




===================================================
Possible race between access to:
        fe->status : drivers/media/dvb/dvb-core/dvb_frontend.c:602 and
        fe->status : drivers/media/dvb/dvb-core/dvb_frontend.c:602
        Accessed at locs:
        drivers/media/dvb/dvb-core/dvb_frontend.c:380 and
        drivers/media/dvb/dvb-core/dvb_frontend.c:413
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {fe->sem#tbd, } (1)
Th. 1 spawned: drivers/media/dvb/dvb-core/dvb_frontend.c:626 w/ func: dvb_frontend_thread
Th. 2 spawned: drivers/media/dvb/dvb-core/dvb_frontend.c:626 w/ func: dvb_frontend_thread


----------
FIRST :

/*
 * FIXME: use linux/kthread.h
 */
static int dvb_frontend_thread(void *data)
{
	struct dvb_frontend *fe = data;
	struct dvb_frontend_private *fepriv = fe->frontend_priv;
	
	lock_kernel();
	daemonize(name);
	sigfillset(&current->blocked);
	unlock_kernel();

	fepriv->status = 0;          <<<< FIRST

	dvb_frontend_init(fe);

	fepriv->wakeup = 0;

	while (1) {
		up(&fepriv->sem);	    /* is locked when we enter the thread... */

        ...

		if (down_interruptible(&fepriv->sem))
			break;

        ...


		/* get the frontend status */
		if (fepriv->state & FESTATE_RETUNE) {
			s = 0;
		} else {
			if (fe->ops->read_status)
				fe->ops->read_status(fe, &s);

			if (s != fepriv->status) {            <<<< SECOND
				dvb_frontend_add_event(fe, s);
				fepriv->status = s;
			}
		}

        ...
	}
    ...
}

hmm... fepriv->sem is supposed to be held on entry... even though this
is a start of a NEW thread


REASON: I guess semaphore counts are global and "inherited"?

(e.g., producer/consumer has one thread locking, and the other unlocking)


===================================================
Possible race between access to:
        (per_cpu__runqueues.expired)->nr_active : kernel/sched.c:262 and
        (per_cpu__runqueues.expired)->nr_active : kernel/sched.c:262
        Accessed at locs:
        kernel/sched.c:3050 and
        kernel/sched.c:611
        Possible paths & LS (first 3):



(0)
        LS for 1st access:
L+ = empty;
        made empty at: kernel/sched.c:3102
        LS for 2nd access:
L+ = {per_cpu__runqueues.lock#tbd, } (1)
ot)->d_inode)->i_mapping)->tree_lock#tbd, } (607)
        made empty at: :-1
        Th. 1 spawned: drivers/block/loop.c:832 w/ func: loop_thread
        Th. 2 spawned: init/main.c:394 w/ func: init

HOLY COW that's a big lockset...

(1)
        LS for 1st access:
L+ = empty;
        made empty at: kernel/sched.c:3102
        LS for 2nd access:
L+ = empty;
        made empty at: kernel/sched.c:1394
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: init/main.c:394 w/ func: init



Hmm... per_cpu in var name, but it's also in the lock name



---------
FIRST :


/*
 * schedule() is the main scheduler function.
 */
asmlinkage void __sched schedule(void)
{
	runqueue_t *rq;
	prio_array_t *array;

    ...

need_resched:
	preempt_disable();
	prev = current;
	release_kernel_lock(prev);
need_resched_nonpreemptible:
	rq = this_rq();

    ...

	spin_lock_irq(&rq->lock);

    ...

	cpu = smp_processor_id();

    ...

	array = rq->active;
	if (unlikely(!array->nr_active)) {             <<<< READ HERE
		/*
		 * Switch the active and expired arrays.
		 */
		schedstat_inc(rq, sched_switch);
		rq->active = rq->expired;
		rq->expired = array;
		array = rq->active;
		rq->expired_timestamp = 0;
		rq->best_expired_prio = MAX_PRIO;
	}

    ...

	if (likely(prev != next)) {
		next->timestamp = now;
		rq->nr_switches++;
		rq->curr = next;
		++*switch_count;

		prepare_task_switch(rq, next);
		prev = context_switch(rq, prev, next);
		barrier();
		/*
		 * this_rq must be evaluated again because prev may have moved
		 * CPUs since it called schedule(), thus the 'rq' on its stack
		 * frame will be invalid.
		 */
		finish_task_switch(this_rq(), prev);
	} else
		spin_unlock_irq(&rq->lock);

    ...

    lock is temporarily lost, but reacquired from the goto?

    ...

	prev = current;
	if (unlikely(reacquire_kernel_lock(prev) < 0))
		goto need_resched_nonpreemptible;

	preempt_enable_no_resched();
	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
		goto need_resched;
}



REASON: lost a lock on our end (or didn't record the loc. of offending access)


---------
SECOND :

static void enqueue_task(struct task_struct *p, prio_array_t *array)
{
	sched_info_queued(p);
	list_add_tail(&p->run_list, array->queue + p->prio);
	__set_bit(p->prio, array->bitmap);

	array->nr_active++;     <<<< HERE

	p->array = array;
}



Should be ok since it's called w/ the lock held (at least in the first
thread spawn scenario)



REASON: our problem for losing a lock that should be held?


===================================================
Possible race between access to:
        ((socket->dev.kobj.kset)->dentry)->d_lru.prev : drivers/pcmcia/cs.c:178 and
        ((socket->cb_dev)->subordinate)->devices.prev : drivers/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/list.h:165 and
        include/linux/list.h:80
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(socket->dev.kobj.dentry)->d_lock#tbd, ((socket->dev.kobj.dentry)->d_inode)->i
_sem#tbd, ((socket->dev.kobj.kset)->dentry)->d_lock#tbd, (((socket->dev.kobj.kset)->
dentry)->d_inode)->i_sem#tbd, } (4)
        LS for 2nd access:
L+ = {(socket->dev.kobj.kset)->list_lock#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd


REASON: lvals + locks wildly different.. (*.kset doesn't have a dentry field)


-------------------------------------
Proof that they are different lvals ?




===================================================
Possible race between access to:
        (c->nextblock)->dirty_size : fs/jffs2/background.c:33 and
        (c->erase_complete_list.next)->dirty_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/nodemgmt.c:175 and
        fs/jffs2/debug.c:28
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: fs/jffs2/nodemgmt.c:324
        LS for 2nd access:
L+ = {c->erase_completion_lock#tbd, c->erase_free_sem#tbd, } (2)
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread



REASON: possibly different lvals (like earlier jffs2 reports)



===================================================
Possible race between access to:
        socket->dev.kobj.kref.refcount.counter : drivers/pcmcia/cs.c:178 and
        (((socket->cb_dev)->subordinate)->devices.next)->dev.kobj.kref.refcount.coun
ter : drivers/pcmcia/cs.c:178
        Accessed at locs:
        lib/kref.c:23 and
        lib/kref.c:32
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(1)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = empty;
        made empty at: drivers/base/class.c:587
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(2)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {socket->skt_sem#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd


REASON: different lvals (well, unless ptr arith makes it the same)


===================================================
Possible race between access to:
((tr->blkcore_priv)->rq)->unplug_timer.entry.next : drivers/mtd/mtd_blkdevs.c:372 and
((tr->blkcore_priv)->rq)->queue_head.next : drivers/mtd/mtd_blkdevs.c:372
        Accessed at locs:
        include/linux/list.h:151 and
        include/linux/list.h:255
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {((journal->j_commit_timer)->base)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
Th. 1 spawned: fs/jbd/journal.c:215 w/ func: kjournald
Th. 2 spawned: fs/jbd/journal.c:215 w/ func: kjournald



(1)
        LS for 1st access:
L+ = {(((tr->blkcore_priv)->rq)->unplug_timer.base)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
Th. 1 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_thread
Th. 2 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_thread


REASON: different lvals (definite from memory layout)



===================================================
Possible race between access to:
        (event_cachep->nodelists[0])->slabs_free.next : fs/inotify.c:43 and
        (event_cachep->nodelists[0])->slabs_free.next : fs/inotify.c:43
        Accessed at locs:
        include/linux/list.h:164 and
        mm/slab.c:2412
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = empty;
        made empty at: mm/slab.c:2427
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm

(1)
        LS for 1st access:
L+ = {(filp_cachep->nodelists[0])->list_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: mm/slab.c:2427
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm

(2)
        LS for 1st access:
L+ = empty;
        made empty at: fs/inotify.c:257
        LS for 2nd access:
L+ = empty;
        made empty at: mm/slab.c:2669
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm


---------
FIRST :

static inline void list_del(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
	entry->next = LIST_POISON1;
	entry->prev = LIST_POISON2;
}

called by... see below

---------
SECOND :


static void *cache_alloc_refill(kmem_cache_t *cachep, gfp_t flags)
{
	int batchcount;
	struct kmem_list3 *l3;
	struct array_cache *ac;

	check_irq_off();
	ac = ac_data(cachep);
retry:

	...

	l3 = cachep->nodelists[numa_node_id()];

	BUG_ON(ac->avail > 0 || !l3);
	spin_lock(&l3->list_lock);

    ...

	while (batchcount > 0) {
		struct list_head *entry;
		struct slab *slabp;
		/* Get slab alloc is to come from. */
		entry = l3->slabs_partial.next;
		if (entry == &l3->slabs_partial) {
			l3->free_touched = 1;

			entry = l3->slabs_free.next;         <<<< READ HERE

			if (entry == &l3->slabs_free)
				goto must_grow;
		}

    ...


		check_slabp(cachep, slabp);

		/* move slabp to correct slabp list: */
		list_del(&slabp->list);                         <<<< FIRST?
		if (slabp->free == BUFCTL_END)
			list_add(&slabp->list, &l3->slabs_full);
		else
			list_add(&slabp->list, &l3->slabs_partial);
	}


must_grow:
	l3->free_objects -= ac->avail;
alloc_done:
	spin_unlock(&l3->list_lock);

    ...

}



REASON: In this area, list_lock is still held -- not sure about other accesses

/*
 * Caller needs to acquire correct kmem_list's list_lock
 */
static void free_block(kmem_cache_t *cachep, void **objpp, int nr_objects, int node)
{
    ...
    slabp = page_get_slab(virt_to_page(objp));
    list_del(&slabp->list);
    ...
}

not sure if all callers of that hold the lock though?



===================================================
Possible race between access to:
        (c->nextblock)->free_size : fs/jffs2/background.c:33 and
        (c->blocks)->free_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/nodemgmt.c:330 and
        fs/jffs2/wbuf.c:163


(0)
        LS for 1st access:
L+ = empty;
        made empty at: fs/jffs2/nodemgmt.c:324
        LS for 2nd access:
L+ = {c->erase_completion_lock#tbd, } (1)
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


REASON: possibly different lvals (like earlier jffs2 reports)



===================================================
Possible race between access to:
((((((((init_task.tasks.next)->mm)->mm_rb.rb_node)->vm_next)->vm_file)->f_vfsmnt)->mnt_root)->d_inode)->i_bdev : include/linux/sched.h:999 and
((((((((init_task.tasks.next)->mm)->mm_rb.rb_node)->vm_next)->vm_file)->f_vfsmnt)->mnt_root)->d_inode)->i_bdev : include/linux/sched.h:999
        Accessed at locs:
        fs/block_dev.c:279 and
        fs/inode.c:261
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {bdev_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/inode.c:266
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/block/loop.c:832 w/ func: loop_thread

(1)
        LS for 1st access:
L+ = {bdev_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/inode.c:266
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/base/firmware_class.c:589 w/ func: request_firmware_w
ork_func

(2)
        LS for 1st access:
L+ = {bdev_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/inode.c:266
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm


REASON: call chain looks long (just based on lval access path) -- skip for now



===================================================
Possible race between access to:
        (c->erase_pending_list.next)->first_node : fs/jffs2/background.c:33 and
        (c->blocks)->first_node : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/erase.c:292 and
        fs/jffs2/nodemgmt.c:709
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->erase_completion_lock#tbd, c->erase_free_sem#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/wbuf.c:402
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


REASON: possibly different lvals (like earlier jffs2 reports)

===================================================
Possible race between access to:
        (c->nextblock)->wasted_size : fs/jffs2/background.c:33 and
        (c->erase_complete_list.next)->wasted_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/nodemgmt.c:329 and
        fs/jffs2/erase.c:380
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: fs/jffs2/nodemgmt.c:324
        LS for 2nd access:
L+ = {c->erase_free_sem#tbd, } (1)


REASON: possibly different lvals (like earlier jffs2 reports)



===================================================
Possible race between access to:
        vmlist->next : include/linux/vmalloc.h:67 and
        vmlist->next : include/linux/vmalloc.h:67
        Accessed at locs:
        mm/vmalloc.c:272 and
        mm/vmalloc.c:264
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: mm/vmalloc.c:296
        LS for 2nd access:
L+ = empty;
        made empty at: mm/vmalloc.c:278
Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
Th. 2 spawned: drivers/base/firmware_class.c:589 w/ func: request_firmware_work_func

(1)
        LS for 1st access:
L+ = empty;
        made empty at: mm/vmalloc.c:296
        LS for 2nd access:
L+ = {vmlist_lock#tbd, } (1)
        made empty at: mm/vmalloc.c:278
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm

(2)
        LS for 1st access:
L+ = empty;
        made empty at: mm/vmalloc.c:296
        LS for 2nd access:
L+ = empty;
        made empty at: mm/vmalloc.c:278
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm

--------
BOTH :

/* Caller must hold vmlist_lock */
struct vm_struct *__remove_vm_area(void *addr)
{
	struct vm_struct **p, *tmp;

	for (p = &vmlist ; (tmp = *p) != NULL ;p = &tmp->next) { <<<< SECOND
		 if (tmp->addr == addr)
			 goto found;
	}
	return NULL;

found:
	unmap_vm_area(tmp);
	*p = tmp->next;                   <<<< FIRST

	/*
	 * Remove the guard page.
	 */
	tmp->size -= PAGE_SIZE;
	return tmp;
}

*p was read and written there

Called by:

struct vm_struct *remove_vm_area(void *addr)
{
	struct vm_struct *v;
	write_lock(&vmlist_lock);
	v = __remove_vm_area(addr);
	write_unlock(&vmlist_lock);
	return v;
}

both have the lock (since both are in the same func, but separated by little)?


only thing in between is a call to:

void unmap_vm_area(struct vm_struct *area)
{
	pgd_t *pgd;
	unsigned long next;
	unsigned long addr = (unsigned long) area->addr;
	unsigned long end = addr + area->size;

	BUG_ON(addr >= end);
	pgd = pgd_offset_k(addr);
	flush_cache_vunmap(addr, end);
	do {
		next = pgd_addr_end(addr, end);
		if (pgd_none_or_clear_bad(pgd))
			continue;
		vunmap_pud_range(pgd, addr, next);
	} while (pgd++, addr = next, addr != end);
	flush_tlb_kernel_range((unsigned long) area->addr, end);
}



REASON: not sure... (or loc of offending access not recorded)?


===================================================
Possible race between access to:
        xtime_lock.sequence : include/linux/time.h:76 and
        xtime_lock.sequence : include/linux/time.h:76
        Accessed at locs:
        include/linux/seqlock.h:53 and
        include/linux/seqlock.h:78
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {xtime_lock.lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: mm/mmap.c:1746
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/block/loop.c:832 w/ func: loop_thread

(1)
        LS for 1st access:
L+ = {xtime_lock.lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: mm/mmap.c:1746
Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
Th. 2 spawned: drivers/base/firmware_class.c:589 w/ func: request_firmware_work_func

(2)
        LS for 1st access:
L+ = {xtime_lock.lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: mm/mmap.c:1746
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm



--------
FIRST:

/* Lock out other writers and update the count.
 * Acts like a normal spin_lock/unlock.
 * Don't need preempt_disable() because that is in the spin_lock already.
 */
static inline void write_seqlock(seqlock_t *sl)
{
	spin_lock(&sl->lock);
	++sl->sequence;          <<<< HERE
	smp_wmb();			
}


--------
SECOND:

/* Start of read calculation -- fetch last complete writer token */
static inline unsigned read_seqbegin(const seqlock_t *sl)
{
	unsigned ret = sl->sequence;   <<<< HERE
	smp_rmb();
	return ret;
}


REASON: memory barriers follows immediately... the read is probably effectless?



===================================================
Possible race between access to:
        (((socket->dev.kobj.kset)->dentry)->d_inode)->i_sb_list.next : drivers/pcmci
a/cs.c:178 and
        ((socket->cb_dev)->subordinate)->devices.next : drivers/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/list.h:223 and
        drivers/pcmcia/cardbus.c:220
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.dentry)->
d_inode)->i_data.assoc_mapping)->private_lock#tbd, (((socket->dev.kobj.kset)->dentry
)->d_inode)->i_sem#tbd, ((((socket->dev.kobj.kset)->dentry)->d_inode)->i_data.assoc_
mapping)->private_lock#tbd, } (4)

...

        LS for 2nd access:
L+ = {socket->skt_sem#tbd, } (1)


        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

REASON: Different lvals + different locks (and kset doesn't have dentry)

lots of warnings spawning from pccardd?



==================================================
Possible race between access to:
        _a137_408448_mempool->tk_magic : mm/mempool.c:115 and
        _a137_408448_mempool->tk_magic : mm/mempool.c:115
        Accessed at locs:
        net/sunrpc/sched.c:786 and
        net/sunrpc/sched.c:359
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {childq.lock#tbd, } (1)
        Th. 1 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread
        Th. 2 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread

(1)
        LS for 1st access:
L+ = empty;
        made empty at: fs/lockd/clntproc.c:364
        LS for 2nd access:
L+ = {childq.lock#tbd, } (1)
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: fs/nfs/nfs4state.c:755 w/ func: reclaimer

(2)
        LS for 1st access:
L+ = empty;
        made empty at: fs/lockd/clntproc.c:364
        LS for 2nd access:
L+ = {childq.lock#tbd, } (1)
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread


---------
FIRST :

/*
 * Creation and deletion of RPC task structures
 */
void rpc_init_task(struct rpc_task *task, struct rpc_clnt *clnt, rpc_action callback, int flags)
{
	memset(task, 0, sizeof(*task));

    ...

#ifdef RPC_DEBUG
	task->tk_magic = RPC_TASK_MAGIC_ID;    <<<< HERE
	task->tk_pid = rpc_task_id++;
#endif
	/* Add to global list of all tasks */
	spin_lock(&rpc_sched_lock);
	list_add_tail(&task->tk_task, &all_tasks);
	spin_unlock(&rpc_sched_lock);

}

During an init (the caller does the alloc):

/*
 * Create a new task for the specified client.  We have to
 * clean up after an allocation failure, as the client may
 * have specified "oneshot".
 */
struct rpc_task *
rpc_new_task(struct rpc_clnt *clnt, rpc_action callback, int flags)
{
	struct rpc_task	*task;

	task = rpc_alloc_task();   <<<< ALLOC!
	if (!task)
		goto cleanup;

	rpc_init_task(task, clnt, callback, flags); <<<< INIT!

	/* Replace tk_release */
	task->tk_release = rpc_default_free_task;

	dprintk("RPC: %4d allocated task\n", task->tk_pid);
	task->tk_flags |= RPC_TASK_DYNAMIC;
out:
	return task;

cleanup:
	/* Check whether to release the client */
	if (clnt) {
		printk("rpc_new_task: failed, users=%d, oneshot=%d\n",
			atomic_read(&clnt->cl_users), clnt->cl_oneshot);
		atomic_inc(&clnt->cl_users); /* pretend we were used ... */
		rpc_release_client(clnt);
	}
	goto out;
}


---------
SECOND :

doesn't matter... it was a read somewhere...

---------


REASON: memory just allocated (before it was thrown onto a global list)



===================================================
Possible race between access to:
(((journal->j_dev)->bd_inode)->i_mapping)->page_tree.height : fs/jbd/journal.c:213 and
(((((journal->j_running_transaction)->t_reserved_list)->b_bh)->b_page)->mapping)->page_tree.height : fs/jbd/journal.c:213
        Accessed at locs:
        lib/radix-tree.c:179 and
        lib/radix-tree.c:359
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(((journal->j_dev)->bd_inode)->i_mapping)->tree_lock#tbd, } (1)
        LS for 2nd access:
L+ = {swapper_space.tree_lock#tbd, (((journal->j_sb_buffer)->b_page)->mapping)->tree
_lock#tbd, } (2)

        Th. 1 spawned: fs/jbd/journal.c:215 w/ func: kjournald
        Th. 2 spawned: fs/jbd/journal.c:215 w/ func: kjournald

(1)
        LS for 1st access:
L+ = {(((journal->j_dev)->bd_inode)->i_mapping)->tree_lock#tbd, } (1)
        LS for 2nd access:
L+ = {swapper_space.tree_lock#tbd, } (1)
        Th. 1 spawned: fs/jbd/journal.c:215 w/ func: kjournald
        Th. 2 spawned: fs/jbd/journal.c:215 w/ func: kjournald

(2)
        LS for 1st access:
L+ = {(((journal->j_dev)->bd_inode)->i_mapping)->tree_lock#tbd, } (1)
        LS for 2nd access:
L+ = {swapper_space.tree_lock#tbd, journal->j_list_lock#tbd, journal->j_state_lock#t
bd, (((((journal->j_running_transaction)->t_reserved_list)->b_bh)->b_page)->mapping)
->tree_lock#tbd, } (4)


REASON: different lvals + locks (but unifiable)



===================================================
Possible race between access to:
(((socket->dev.kobj.kset)->dentry)->d_inode)->i_sb_list.next : drivers/pcmcia/cs.c:178 and
socket->cis_cache.next : drivers/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/list.h:223 and
        include/linux/list.h:164
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.dentry)->
d_inode)->i_data.assoc_mapping)->private_lock#tbd, (((socket->dev.kobj.kset)->dentry
)->d_inode)->i_sem#tbd, ((((socket->dev.kobj.kset)->dentry)->d_inode)->i_data.assoc_
mapping)->private_lock#tbd, } (4)
        LS for 2nd access:
L+ = {socket->skt_sem#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(1)
        LS for 1st access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.kset)->de
ntry)->d_inode)->i_sem#tbd, } (2)
        made empty at: fs/inode.c:266
        LS for 2nd access:
L+ = {socket->skt_sem#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(2)
        LS for 1st access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.kset)->de
ntry)->d_inode)->i_sem#tbd, } (2)
        LS for 2nd access:
L+ = {socket->skt_sem#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd


REASON: different lvals (unlikely from memory layout)

--------------

Basically, ((socket->dev.kobj.kset)->dentry)->d_node  has to point to
*(socket - offset of i_sb_list + offset of cis_cache)




===================================================
Possible race between access to:
((init_task.mm)->ioctx_list)->wq.timer.entry.next : include/linux/sched.h:999 and
((init_task.mm)->ioctx_list)->wq.timer.entry.next : include/linux/sched.h:999
        Accessed at locs:
        include/linux/timer.h:57 and
        include/linux/list.h:151
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: kernel/timer.c:333
        LS for 2nd access:
L+ = empty;
        made empty at: mm/oom_kill.c:286
        Th. 1 spawned: drivers/block/loop.c:832 w/ func: loop_thread
        Th. 2 spawned: init/main.c:394 w/ func: init

(1)
        LS for 1st access:
L+ = empty;
        made empty at: kernel/timer.c:302
        LS for 2nd access:
L+ = {(console_timer.base)->lock#tbd, } (1)
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: init/main.c:394 w/ func: init

(2)
        LS for 1st access:
L+ = empty;
        made empty at: kernel/timer.c:333
        LS for 2nd access:
L+ = empty;
        made empty at: mm/oom_kill.c:286
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: init/main.c:394 w/ func: init



extern struct task_struct init_task;


--------
FIRST :

static inline int timer_pending(const struct timer_list * timer)
{
	return timer->entry.next != NULL;   <<< HMM...
}


where was the real access (by user of timer lib)? Let's look at where
it was made empty?


User 1: timer.c

int del_timer(struct timer_list *timer)
{
	timer_base_t *base;
	unsigned long flags;
	int ret = 0;

	if (timer_pending(timer)) {                    <<<< HERE
		base = lock_timer_base(timer, &flags);     <<<< LOCKS
		if (timer_pending(timer)) {                <<<< HERE
			detach_timer(timer, 1);
			ret = 1;
		}
		spin_unlock_irqrestore(&base->lock, flags);
	}

	return ret;
}

Double-checked locking

User 2: timer.c

int mod_timer(struct timer_list *timer, unsigned long expires)
{
	BUG_ON(!timer->function);

	/*
	 * This is a common optimization triggered by the
	 * networking code - if the timer is re-modified
	 * to be the same thing then just return:
	 */
	if (timer->expires == expires && timer_pending(timer))   <<<< HERE
		return 1;

	return __mod_timer(timer, expires);  <<<< CALLS THIS
}

int __mod_timer(struct timer_list *timer, unsigned long expires)
{
	timer_base_t *base;
	tvec_base_t *new_base;
	unsigned long flags;
	int ret = 0;

	BUG_ON(!timer->function);

	base = lock_timer_base(timer, &flags);   <<<< LOCKS

	if (timer_pending(timer)) {              <<<< CHECKS AGAIN!
		detach_timer(timer, 0);
		ret = 1;
	}

    ...

}

Double-checked locking

--------
SECOND :

static inline void __list_del(struct list_head * prev, struct list_head * next)
{
	next->prev = prev;
	prev->next = next;
}

called by?

User 1: mm/oom_kill.c:286

/**
 * oom_kill - kill the "best" process when we run out of memory
 */
void out_of_memory(gfp_t gfp_mask, int order)
{
    struct mm_struct *mm = NULL;
	task_t * p;

	if (printk_ratelimit()) {
		printk("oom-killer: gfp_mask=0x%x, order=%d\n",
			gfp_mask, order);
		show_mem();
	}

	read_lock(&tasklist_lock);
retry:
	p = select_bad_process();

	if (PTR_ERR(p) == -1UL)
		goto out;

	/* Found nothing?!?! Either we hang forever, or we panic. */
	if (!p) {
		read_unlock(&tasklist_lock);
		panic("Out of memory and no killable processes...\n");
	}

	mm = oom_kill_process(p);
	if (!mm)
		goto retry;

 out:
	read_unlock(&tasklist_lock);
	if (mm)
		mmput(mm);

	/*
	 * Give "p" a good chance of killing itself before we
	 * retry to allocate memory.
	 */
	schedule_timeout_interruptible(1);   <<<< HERE?
}


REASON: Double-checked locking pattern


===================================================
Possible race between access to:
        mount_hashtable : fs/namespace.c:45 and
        mount_hashtable : fs/namespace.c:45
        Accessed at locs:
        include/linux/list.h:164 and
        fs/namespace.c:97
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, vfsmount_lock#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/namei.c:666
Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread
        Th. 2 spawned: fs/nfs/nfs4state.c:755 w/ func: reclaimer

(1)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, vfsmount_lock#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/namei.c:666
Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread
        Th. 2 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread

(2)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, vfsmount_lock#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/namei.c:666
Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread
        Th. 2 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer


/// some other info
static struct list_head *mount_hashtable; (in namespace.c)
/* spinlock for vfsmount related operations, inplace of dcache_lock */
__cacheline_aligned_in_smp DEFINE_SPINLOCK(vfsmount_lock);



--------
FIRST :


static inline void list_del(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
	entry->next = LIST_POISON1;            <<<< HERE
	entry->prev = LIST_POISON2;
}

called by? no middle reference point?!... oh well... lock is held at least



--------
SECOND :

struct vfsmount *__lookup_mnt(struct vfsmount *mnt, struct dentry *dentry,
			      int dir)
{
	struct list_head *head = mount_hashtable + hash(mnt, dentry);  <<<< HERE

	struct list_head *tmp = head;
	struct vfsmount *p, *found = NULL;

	for (;;) {
		tmp = dir ? tmp->next : tmp->prev;
		p = NULL;
		if (tmp == head)
			break;
		p = list_entry(tmp, struct vfsmount, mnt_hash);
		if (p->mnt_parent == mnt && p->mnt_mountpoint == dentry) {
			found = p;
			break;
		}
	}
	return found;
}

which is called by:


/*
 * lookup_mnt increments the ref count before returning
 * the vfsmount struct.
 */
struct vfsmount *lookup_mnt(struct vfsmount *mnt, struct dentry *dentry)
{
	struct vfsmount *child_mnt;

	spin_lock(&vfsmount_lock);       <<<< LOCK
	if ((child_mnt = __lookup_mnt(mnt, dentry, 1)))   <<<< CALL
		mntget(child_mnt);
	spin_unlock(&vfsmount_lock);     <<<<

	return child_mnt;
}

Lock should actually be held...

Why was it made empty at: fs/namei.c:666 ???

static void follow_mount(struct vfsmount **mnt, struct dentry **dentry)
{
	while (d_mountpoint(*dentry)) {
		struct vfsmount *mounted = lookup_mnt(*mnt, *dentry);  <<<< OK

		if (!mounted)
			break;
		dput(*dentry);

		mntput(*mnt);    <<< shady?
		*mnt = mounted;  <<< what was this called with?

		*dentry = dget(mounted->mnt_root);
	}
}

static inline void follow_dotdot(struct nameidata *nd)
{
	while(1) {
		struct vfsmount *parent;
		struct dentry *old = nd->dentry;

        ...

		nd->mnt = parent;
	}

    follow_mount(&nd->mnt, ...)
}


REASON: unsure about second access... recorded reference points look fine


===================================================
Possible race between access to:
        pktgen_threads : net/core/pktgen.c:495 and
        pktgen_threads : net/core/pktgen.c:495
        Accessed at locs:
        net/core/pktgen.c:2521 and
        net/core/pktgen.c:2514
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {pktgen_sem#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: net/core/pktgen.c:2942 w/ func: pktgen_thread_worker
        Th. 2 spawned: net/core/pktgen.c:2942 w/ func: pktgen_thread_worker


------------------
FIRST AND SECOND :

static struct pktgen_thread *pktgen_threads = NULL;

static void pktgen_rem_thread(struct pktgen_thread *t) 
{
        /* Remove from the thread list */

	struct pktgen_thread *tmp = pktgen_threads;   <<<< R SECOND

	remove_proc_entry(t->name, pg_proc_dir);

	thread_lock();     <<<< LOCK

	if (tmp == t)
		pktgen_threads = tmp->next;   <<<< W FIRST
	else {
		while (tmp) {
			if (tmp->next == t) {
				tmp->next = t->next;
				t->next = NULL;
				break;
			}
			tmp = tmp->next;
		}
	}
        thread_unlock();  <<<< UNLOCK
}


REASON: Looks like a real bug (and they changed it)

FOLLOW-UP: by version 2.6.17 they re-wrote the code to be safer

static void pktgen_rem_thread(struct pktgen_thread *t)
{
	/* Remove from the thread list */

	remove_proc_entry(t->name, pg_proc_dir);

	mutex_lock(&pktgen_thread_lock);

	list_del(&t->th_list);               <<<< rather than custom code above

	mutex_unlock(&pktgen_thread_lock);
}

not sure how atomic splitting the two removes is though...




===================================================
Possible race between access to:
((skbuff_fclone_cache->nodelists[0])->shared)->avail : net/core/skbuff.c:72 
and
((skbuff_fclone_cache->nodelists[0])->shared)->avail : net/core/skbuff.c:72
        Accessed at locs:
        mm/slab.c:2706 and
        mm/slab.c:2393
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(skbuff_head_cache->nodelists[0])->list_lock#tbd, } (1)
        LS for 2nd access:
L+ = {(skbuff_fclone_cache->nodelists[0])->list_lock#tbd, } (1)
Th. 1 spawned: drivers/net/irda/stir4200.c:931 w/ func: stir_transmit_thread
Th. 2 spawned: init/main.c:394 w/ func: init

(1)
        LS for 1st access:
L+ = {(skbuff_head_cache->nodelists[0])->list_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
L- = {..., (skbuff_fclone_cache->nodelists[0])->list_lock, ... }
        made empty at: net/core/skbuff.c:182
Th. 1 spawned: drivers/net/irda/stir4200.c:931 w/ func: stir_transmit_thread
Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(2)
        LS for 1st access:
L+ = {(skbuff_head_cache->nodelists[0])->list_lock#tbd, } (1)
        LS for 2nd access:
L+ = {(skbuff_fclone_cache->nodelists[0])->list_lock#tbd, } (1)
Th. 1 spawned: drivers/net/irda/stir4200.c:931 w/ func: stir_transmit_thread
Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd



--------
FIRST :

static void cache_flusharray(kmem_cache_t *cachep, struct array_cache *ac)
{
    ...

	l3 = cachep->nodelists[node];
	spin_lock(&l3->list_lock);                 <<<< LOCK
	if (l3->shared) {
		struct array_cache *shared_array = l3->shared;
		int max = shared_array->limit-shared_array->avail;
		if (max) {
			if (batchcount > max)
				batchcount = max;

			memcpy(&(shared_array->entry[shared_array->avail]),  <<<< HERE
					ac->entry,
					sizeof(void*)*batchcount);

			shared_array->avail += batchcount;  <<<< HERE
			goto free_done;
		}
	}

	free_block(cachep, ac->entry, batchcount, node);
free_done:

    ...
	spin_unlock(&l3->list_lock);

    ...

}



---------
SECOND :

static void *cache_alloc_refill(kmem_cache_t *cachep, gfp_t flags)
{
	int batchcount;
	struct kmem_list3 *l3;
	struct array_cache *ac;

    ...

	l3 = cachep->nodelists[numa_node_id()];

	spin_lock(&l3->list_lock);

	if (l3->shared) {
		struct array_cache *shared_array = l3->shared;

		if (shared_array->avail) {                <<<< HERE

			if (batchcount > shared_array->avail) <<<< HERE

				batchcount = shared_array->avail; <<<< HERE
			shared_array->avail -= batchcount;    <<<< HERE
			ac->avail = batchcount;
			memcpy(ac->entry,
				&(shared_array->entry[shared_array->avail]), <<<< HERE
				sizeof(void*)*batchcount);
			shared_array->touched = 1;
			goto alloc_done;
		}
	}
    
    ...

must_grow:
	l3->free_objects -= ac->avail;
alloc_done:
	spin_unlock(&l3->list_lock);
    ...
}


Need to look at another access (not made empty by this place?)


made empty at: net/core/skbuff.c:182

struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
			    int fclone)
{
	struct sk_buff *skb;
	u8 *data;

	/* Get the HEAD */
	if (fclone)
		skb = kmem_cache_alloc(skbuff_fclone_cache,
				       gfp_mask & ~__GFP_DMA);         <<<< HERE?!


	else
		skb = kmem_cache_alloc(skbuff_head_cache,      <<<< Mixing w/ this?
				       gfp_mask & ~__GFP_DMA);

    ...
}

the kmem_cache_alloc call will do the locking further down (it leads
to the function with the l3->list_lock)


REASON: accesses look fine, but there may be a bad one somewhere else or
alias analysis may have failed to resolve a lock somewhere (leaving it in
the L- set though)?)

Even if both accesses have locks, need to watch out for bogus comparison of
"skbuff_fclone_cache" accesses against "skbuff_head_cache"



===================================================
Possible race between access to:
        (rxrpc_krxiod_callq.next)->app_ready_qty : net/rxrpc/krxiod.c:30 and
        (rxrpc_krxiod_callq.next)->app_ready_qty : net/rxrpc/krxiod.c:30
        Accessed at locs:
        net/rxrpc/call.c:1084 and
        net/rxrpc/call.c:1079
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(rxrpc_krxiod_callq.next)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
    made empty at: net/rxrpc/call.c:1266
        Th. 1 spawned: net/rxrpc/krxiod.c:159 w/ func: rxrpc_krxiod
        Th. 2 spawned: net/rxrpc/krxiod.c:159 w/ func: rxrpc_krxiod


-----------------
FIRST AND SECOND

static void rxrpc_call_receive_data_packet(struct rxrpc_call *call,
					   struct rxrpc_message *msg)
{

...
	/* next in sequence - simply append into the call's ready queue */
	_debug("Call add packet %d to readyq (+%Zd => %Zd bytes)",
	       msg->seq, msg->dsize, call->app_ready_qty);  <<<< SECOND

	spin_lock(&call->lock);
	call->app_ready_seq = msg->seq;
	call->app_ready_qty += msg->dsize;               <<<< FIRST
	list_add_tail(&msg->link, &call->app_readyq);

...

}


REASON: read for printing debug message -- may not want to block on spinlock


===================================================
Possible race between access to:
        (_a918_394856_readinode.inocache)->state : fs/jffs2/readinode.c:915 and
        (_a918_394856_readinode.inocache)->state : fs/jffs2/readinode.c:915
        Accessed at locs:
        fs/jffs2/nodelist.c:858 and
        fs/jffs2/readinode.c:649
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->inocache_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/readinode.c:836
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


---------
FIRST :

void jffs2_set_inocache_state(struct jffs2_sb_info *c, struct jffs2_inode_cache *ic, int state)
{
	spin_lock(&c->inocache_lock);
	ic->state = state;                 <<< HERE
	wake_up(&c->inocache_wq);
	spin_unlock(&c->inocache_lock);
}


---------
SECOND :

static int jffs2_do_read_inode_internal(struct jffs2_sb_info *c,
					struct jffs2_inode_info *f,
					struct jffs2_raw_inode *latest_node)
{

    ...

	/* Grab all nodes relevant to this ino */
	ret = jffs2_get_inode_nodes(c, f, &tn_list, &fd_list, &f->highest_version, &latest_mctime, &mctime_ver);

	if (ret) {

		if (f->inocache->state == INO_STATE_READING)  <<< HERE

			jffs2_set_inocache_state(c, f->inocache, INO_STATE_CHECKEDABSENT);
        
            ^^^^ CALLS FIRST for a write!

		return ret;
	}

    ...
}

REASON: A race, but the change in the state field is idempotent, and so is the call to wake_up(&c...)?

FOLLOWUP: looks like wake_up is idempotent... they only sleep_on_spinunlock()
which does not link a wait_queue callback function... it only makes the
thread ready and reschedules


===================================================
Possible race between access to:
((kafsasyncd_async_attnq.next)->call)->app_call_state : fs/afs/kafsasyncd.c:40
((kafsasyncd_async_attnq.next)->call)->app_call_state : fs/afs/kafsasyncd.c:40
        Accessed at locs:
        net/rxrpc/call.c:862 and
        net/rxrpc/call.c:852
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {((kafsasyncd_async_attnq.next)->call)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: fs/afs/kafsasyncd.c:60 w/ func: kafsasyncd
        Th. 2 spawned: fs/afs/kafsasyncd.c:60 w/ func: kafsasyncd


-------------------
FIRST AND SECOND :

static int __rxrpc_call_abort(struct rxrpc_call *call, int errno)
{
	struct rxrpc_connection *conn = call->conn;
	struct rxrpc_message *msg;
	struct kvec diov[1];
	int ret;
	__be32 _error;


	/* if this call is already aborted, then just wake up any waiters */
	if (call->app_call_state == RXRPC_CSTATE_ERROR) {  <<< FIRST
		spin_unlock(&call->lock);
		call->app_error_func(call);
		_leave(" = 0");
		return 0;
	}

	rxrpc_get_call(call);

	/* change the state _with_ the lock still held */
	call->app_call_state	= RXRPC_CSTATE_ERROR;      <<< SECOND

    ...

	spin_unlock(&call->lock);

    ...
} /* end __rxrpc_call_abort() */


REASON: lost a lock on our end, or offending location not recorded


===================================================
Possible race between access to:
        curr_sb : net/ipv4/ipvs/ip_vs_sync.c:125 and
        ip_vs_sync_queue.next : net/ipv4/ipvs/ip_vs_sync.c:121
        Accessed at locs:
        net/ipv4/ipvs/ip_vs_sync.c:205 and
        include/linux/list.h:255
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {curr_sb_lock#tbd, } (1)
        LS for 2nd access:
L+ = {ip_vs_sync_lock#tbd, } (1)
        Th. 1 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread
        Th. 2 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread


REASON: different lvals + locks (one is a list, other is a buffer)...


static struct ip_vs_sync_buff   *curr_sb = NULL;
static LIST_HEAD(ip_vs_sync_queue);



===================================================
Possible race between access to:
        (c->erase_pending_list.next)->used_size : fs/jffs2/background.c:33 and
        (c->blocks)->used_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/erase.c:138 and
        fs/jffs2/nodemgmt.c:518
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->erase_completion_lock#tbd, c->erase_free_sem#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/wbuf.c:402
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


REASON: possibly different lvals (another jffs2 gc thing)


===================================================
Possible race between access to:
socket->dev.node.next : drivers/pcmcia/cs.c:178 and
((socket->dev.kobj.kset)->dentry)->d_subdirs.next : drivers/pcmcia/cs.c:178
        Accessed at locs:
        drivers/base/class.c:475 and
        include/linux/list.h:67
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: drivers/pcmcia/cs.c:699
        LS for 2nd access:
L+ = {socket->thread_wait.lock#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(1)
        LS for 1st access:
L+ = empty;
        made empty at: drivers/pcmcia/cs.c:699
        LS for 2nd access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(2)
        LS for 1st access:
L+ = empty;
        made empty at: drivers/pcmcia/cs.c:699
        LS for 2nd access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.kset)->de
ntry)->d_inode)->i_sem#tbd, } (2)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd


REASON: different lvals (based memory layout -- unlikely)



===================================================
Possible race between access to:
        ((((((((init_task.tasks.next)->mm)->mm_rb.rb_node)->vm_next)->vm_file)->f_vfsmnt)->mnt_root)->d_inode)->i_mapping : include/linux/sched.h:999 and
        ((((((((init_task.tasks.next)->mm)->mm_rb.rb_node)->vm_next)->vm_file)->f_vfsmnt)->mnt_root)->d_inode)->i_mapping : include/linux/sched.h:999
        Accessed at locs:
        fs/block_dev.c:280 and
        fs/fs-writeback.c:157
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {bdev_lock#tbd, } (1)
        LS for 2nd access:
L+ = {inode_lock#tbd, } (1)
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/block/loop.c:832 w/ func: loop_thread

(1)
        LS for 1st access:
L+ = {bdev_lock#tbd, } (1)
        LS for 2nd access:
L+ = {inode_lock#tbd, } (1)
Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
Th. 2 spawned: drivers/base/firmware_class.c:589 w/ func: request_firmware_work_func

(2)
        LS for 1st access:
L+ = {bdev_lock#tbd, } (1)
        LS for 2nd access:
L+ = {inode_lock#tbd, } (1)
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm


---------
FIRST :

static inline void __bd_forget(struct inode *inode)
{
	list_del_init(&inode->i_devices);
	inode->i_bdev = NULL;
	inode->i_mapping = &inode->i_data;  <<<< HERE
}


Seems like the inode_lock should be held here (in addition to bdev_lock ?)

but looks like it's not held from at least 2 paths...


---------
SECOND :

static int
__sync_single_inode(struct inode *inode, struct writeback_control *wbc)
{
	struct address_space *mapping = inode->i_mapping;   <<<< HERE

...

	spin_unlock(&inode_lock);

	ret = do_writepages(mapping, wbc);

	/* Don't write the inode if only I_DIRTY_PAGES was set */
	if (dirty & (I_DIRTY_SYNC | I_DIRTY_DATASYNC)) {
		int err = write_inode(inode, wait);
		if (ret == 0)
			ret = err;
	}

	if (wait) {
		int err = filemap_fdatawait(mapping);
		if (ret == 0)
			ret = err;
	}

	spin_lock(&inode_lock);
	inode->i_state &= ~I_LOCK;
	if (!(inode->i_state & I_FREEING)) {
		if (!(inode->i_state & I_DIRTY) &&
		    mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
			/*
			 * We didn't write back all the pages.  nfs_writepages()
			 * sometimes bales out without doing anything. Redirty
			 * the inode.  It is still on sb->s_io.
			 */
			if (wbc->for_kupdate) {
				/*
				 * For the kupdate function we leave the inode
				 * at the head of sb_dirty so it will get more
				 * writeout as soon as the queue becomes
				 * uncongested.
				 */
				inode->i_state |= I_DIRTY_PAGES;
				list_move_tail(&inode->i_list, &sb->s_dirty);
			} else {
				/*
				 * Otherwise fully redirty the inode so that
				 * other inodes on this superblock will get some
				 * writeout.  Otherwise heavy writing to one
				 * file would indefinitely suspend writeout of
				 * all the other files.
				 */
				inode->i_state |= I_DIRTY_PAGES;
				inode->dirtied_when = jiffies;
				list_move(&inode->i_list, &sb->s_dirty);
			}
		} else if (inode->i_state & I_DIRTY) {
			/*
			 * Someone redirtied the inode while were writing back
			 * the pages.
			 */
			list_move(&inode->i_list, &sb->s_dirty);
		} else if (atomic_read(&inode->i_count)) {
			/*
			 * The inode is clean, inuse
			 */
			list_move(&inode->i_list, &inode_in_use);
		} else {
			/*
			 * The inode is clean, unused
			 */
			list_move(&inode->i_list, &inode_unused);
		}
	}
	wake_up_inode(inode);
	return ret;
}

BOOKMARK HERE



===================================================
Possible race between access to:
        suspends_pending : arch/i386/kernel/apm.c:390 and
        apm_info.bios.flags : include/linux/apm_bios.h:109
        Accessed at locs:
        arch/i386/kernel/apm.c:1147 and
        arch/i386/kernel/apm.c:1777
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {user_list_lock#tbd, } (1)


...

REASON: different lvals (definite from memory layout)


===================================================
Possible race between access to:
((((init_task.mm)->mm_rb.rb_node)->vm_next)->vm_file)->f_ep_links.next :
 include/linux/sched.h:999 and
((((init_task.mm)->mm_rb.rb_node)->vm_next)->vm_file)->f_ep_links.next :
 include/linux/sched.h:999
        Accessed at locs:
        fs/eventpoll.c:363 and
        include/linux/list.h:255
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {epsem#tbd, } (1)
        made empty at: fs/eventpoll.c:1134
        LS for 2nd access:
L+ = empty;
        made empty at: fs/eventpoll.c:1134
Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
Th. 2 spawned: drivers/block/loop.c:832 w/ func: loop_thread

(1)
        LS for 1st access:
L+ = {epsem#tbd, } (1)
        made empty at: fs/eventpoll.c:1134
        LS for 2nd access:
L+ = empty;
        made empty at: fs/eventpoll.c:1134
Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
Th. 2 spawned: drivers/base/firmware_class.c:589 w/ func: request_firmware_work_func


CHECK it

===================================================
Possible race between access to:
((((dev_index_head[0].first)->ip_ptr)->mc_list)->sources)->sf_count[0] :
 net/core/dev.c:178 and
((((dev_index_head[0].first)->ip_ptr)->mc_list)->sources)->sf_count[0] :
 net/core/dev.c:178
        Accessed at locs:
        net/ipv4/igmp.c:1487 and
        net/ipv4/igmp.c:2143
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(((dev_index_head[0].first)->ip_ptr)->mc_list)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: net/ipv4/igmp.c:1673
        Th. 1 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread
        Th. 2 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread


CHECK it



===================================================
Possible race between access to:
(((socket->dev.kobj.kset)->dentry)->d_inode)->i_sb_list.next : drivers/pcmcia/cs.c:178 and
socket->dev.kobj.entry.next : drivers/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/list.h:223 and
        lib/kobject.c:126
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.dentr
y)->d_inode)->i_data.assoc_mapping)->private_lock#tbd, (((socket->dev.kobj.kset)
->dentry)->d_inode)->i_sem#tbd, ((((socket->dev.kobj.kset)->dentry)->d_inode)->i
_data.assoc_mapping)->private_lock#tbd, } (4)

...

REASON: possibly different lvals (unlikely from memory layout)


===================================================
Possible race between access to:
        ip_vs_sync_queue.next : net/ipv4/ipvs/ip_vs_sync.c:121 and
        curr_sb : net/ipv4/ipvs/ip_vs_sync.c:125
        Accessed at locs:
        include/linux/list.h:164 and
        net/ipv4/ipvs/ip_vs_sync.c:203
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {ip_vs_sync_lock#tbd, } (1)
        LS for 2nd access:
L+ = {curr_sb_lock#tbd, } (1)
        Th. 1 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread
        Th. 2 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread



REASON: different lvals (definite from memory layout)


===================================================
Possible race between access to:
        last_pid : include/linux/sched.h:96 and
        last_pid : include/linux/sched.h:96
        Accessed at locs:
        kernel/pid.c:105 and
        kernel/pid.c:76
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
...

(1)
        LS for 1st access:
L+ = {nlmsvc_sema#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: net/ipv4/ipvs/ip_vs_sync.c:858 w/ func: fork_sync_thread


        LS for 1st access:
L+ = {nlmsvc_sema#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: init/main.c:394 w/ func: init

CHECK it


===================================================
Possible race between access to:
        (*_a137_408448_mempool) : mm/mempool.c:115 and
        (*_a137_408448_mempool) : mm/mempool.c:115
        Accessed at locs:
        include/asm/string.h:425 and
        fs/cifs/transport.c:790
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: fs/cifs/connect.c:2448
        LS for 2nd access:
L+ = {(((GlobalOplock_Q.next)->tcon)->ses)->sesSem#tbd, } (1)
        Th. 1 spawned: fs/cifs/cifsfs.c:980 w/ func: cifs_oplock_thread
        Th. 2 spawned: fs/cifs/cifsfs.c:980 w/ func: cifs_oplock_thread


CHECK this



===================================================
Possible race between access to:
        sync_master_pid : net/ipv4/ipvs/ip_vs_sync.c:622 and
        ip_vs_sync_queue.next : net/ipv4/ipvs/ip_vs_sync.c:121
        Accessed at locs:
        net/ipv4/ipvs/ip_vs_sync.c:729 and
        include/linux/list.h:255
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;


REASON: different lvals (definite from memory layout... ip_vs again)



===================================================
Possible race between access to:
        per_cpu__init_tss.io_bitmap_base : include/asm/processor.h:92 and
        per_cpu__init_tss.io_bitmap_base : include/asm/processor.h:92
        Accessed at locs:
        arch/i386/kernel/process.c:387 and
        arch/i386/kernel/process.c:387
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_th
read
        Th. 2 spawned: drivers/net/irda/sir_kthread.c:482 w/ func: irda_thread

(1)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_th
read
        Th. 2 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_threa
d

...

CHECK this

===================================================
Possible race between access to:
        ((c->blocks)->first_node)->flash_offset : fs/jffs2/background.c:33 and
        ((c->nextblock)->first_node)->flash_offset : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/wbuf.c:373 and
        fs/jffs2/nodemgmt.c:478
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->erase_completion_lock#tbd, } (1)
...

REASON: possibly different lvals (jffs2 thing again)

-----------
Proof ?


===================================================
Possible race between access to:
        (c->nextblock)->dirty_size : fs/jffs2/background.c:33 and
        (c->erase_pending_list.next)->dirty_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/nodemgmt.c:175 and
        fs/jffs2/erase.c:138
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;


... 

REASON: possibly different lvals (jffs2 thing again)


===================================================
Possible race between access to:
        _a137_408448_mempool->tk_workqueue : mm/mempool.c:115 and
        _a137_408448_mempool->tk_workqueue : mm/mempool.c:115
        Accessed at locs:
        net/sunrpc/sched.c:775 and
        net/sunrpc/sched.c:286
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {childq.lock#tbd, } (1)
        Th. 1 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread
        Th. 2 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread

(1)
        LS for 1st access:
L+ = empty;
        made empty at: fs/lockd/clntproc.c:364
        LS for 2nd access:
L+ = {childq.lock#tbd, } (1)
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: fs/nfs/nfs4state.c:755 w/ func: reclaimer

(2)
        LS for 1st access:
L+ = empty;
        made empty at: fs/lockd/clntproc.c:364
        LS for 2nd access:
L+ = {childq.lock#tbd, } (1)
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread


CHECK it

===================================================
Possible race between access to:
        last_pid : include/linux/sched.h:96 and
        last_pid : include/linux/sched.h:96
        Accessed at locs:
        kernel/pid.c:105 and
        kernel/pid.c:105
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;

...


(2)
        LS for 1st access:
L+ = {nlmsvc_sema#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: init/main.c:394 w/ func: init



CHECK: Only once is lock held (and locks aren't made empty for other cases)




===================================================
Possible race between access to:
        (mq->queue)->unplug_timer.entry.next : drivers/mmc/mmc_queue.c:126 and
        mq->thread_complete.wait.task_list.next : drivers/mmc/mmc_queue.c:126
        Accessed at locs:
        include/linux/list.h:151 and
        kernel/sched.c:3240
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {((journal->j_commit_timer)->base)->lock#tbd, } (1)
        LS for 2nd access:
L+ = {journal->j_state_lock#tbd, journal->j_wait_transaction_locked.lock#tbd, } (2)
        Th. 1 spawned: fs/jbd/journal.c:215 w/ func: kjournald
        Th. 2 spawned: fs/jbd/journal.c:215 w/ func: kjournald

(1)
        LS for 1st access:
L+ = {(((tr->blkcore_priv)->rq)->unplug_timer.base)->lock#tbd, } (1)
        LS for 2nd access:
L+ = {(tr->blkcore_priv)->thread_dead.wait.lock#tbd, } (1)
        Th. 1 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_thread
        Th. 2 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_thread

(2)
        LS for 1st access:
L+ = {mq->thread_sem#tbd, ((mq->queue)->unplug_timer.base)->lock#tbd, } (2)
        LS for 2nd access:
L+ = {mq->thread_complete.wait.lock#tbd, } (1)
        Th. 1 spawned: drivers/mmc/mmc_queue.c:161 w/ func: mmc_queue_thread
        Th. 2 spawned: drivers/mmc/mmc_queue.c:161 w/ func: mmc_queue_thread


REASON: possibly different lvals (based on memory layout)

suppose *mq == *mq in both accesses... then *mq.queue != *mq.thread_complete

since there are no more derefs for the second access... and only one more
deref in the first (followed by an incompatible field offset), it's unlikely
they're the same address

can also show that *(mq->queue) is fresh with respect to a part
of mq->thread_complete.blah (which can't be any more fresh since it's part
of the struct)

------------------
Proof ?



int mmc_init_queue(struct mmc_queue *mq,                       <<< this guy
                    struct mmc_card *card, spinlock_t *lock)
{
    struct mmc_host *host = card->host;
	
    ...

	mq->card = card;

	mq->queue = blk_init_queue(mmc_request, lock); <<<< queue set here

	blk_queue_prep_rq(mq->queue, mmc_prep_request); <<<< sets a field of the q

    ...

	init_completion(&mq->thread_complete);  <<<< inits completion part

	ret = kernel_thread(mmc_queue_thread, mq, CLONE_KERNEL);
	if (ret >= 0) {

		wait_for_completion(&mq->thread_complete); <<<< completion stuff again

		init_completion(&mq->thread_complete);  <<<< completion stuff again
        ret = 0;
		goto out;
	}
 cleanup:
	kfree(mq->sg);
	mq->sg = NULL;

	blk_cleanup_queue(mq->queue);   <<<< frees the queue
 out:
	return ret;
}

blk_init_queue returns a fresh cell (but the mechanism seems complex)


===================================================
Possible race between access to:
        per_cpu__mmu_gathers.nr : include/asm-generic/tlb.h:49 and
        per_cpu__mmu_gathers.nr : include/asm-generic/tlb.h:49
        Accessed at locs:
        include/asm-generic/tlb.h:62 and
        include/asm-generic/tlb.h:109
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: mm/memory.c:810
        LS for 2nd access:
L+ = {(per_cpu__mmu_gathers.mm)->page_table_lock#tbd, } (1)
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/block/loop.c:832 w/ func: loop_thread

(1)
        LS for 1st access:
L+ = empty;
        made empty at: mm/memory.c:810
        LS for 2nd access:
L+ = {(per_cpu__mmu_gathers.mm)->page_table_lock#tbd, } (1)
Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
Th. 2 spawned: drivers/base/firmware_class.c:589 w/ func: request_firmware_work_func



CHECK it


===================================================
Possible race between access to:
        (socket->dev.kobj.kset)->entry.next : drivers/pcmcia/cs.c:178 and
        (((socket->dev.kobj.kset)->dentry)->d_inode)->inotify_watches.next : drivers
/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/list.h:223 and
        fs/inotify.c:654
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;


REASON: different lvals (unlikely from memory layout)

-------------
Proof ?

since the prefix is the same, only need to show that 

(x.dentry->d_inode) doesn't point to 
((x.entry) - (bits_offset of inotify_watches))




===================================================
Possible race between access to:
        (((sem_ids.entries)->p[0])->sem_base)->sempid : ipc/sem.c:88 and
        (((sem_ids.entries)->p[0])->sem_base)->sempid : ipc/sem.c:88
        Accessed at locs:
        ipc/sem.c:318 and
        ipc/sem.c:318
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, } (1)

...

CHECK it (another sem.c thing...)

===================================================
Possible race between access to:
        sync_backup_pid : net/ipv4/ipvs/ip_vs_sync.c:623 and
        curr_sb : net/ipv4/ipvs/ip_vs_sync.c:125
        Accessed at locs:
        net/ipv4/ipvs/ip_vs_sync.c:731 and
        net/ipv4/ipvs/ip_vs_sync.c:205
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;


REASON: different lvals (definite from memory layout)


===================================================
Possible race between access to:
        swap_token_mm : include/linux/swap.h:226 and
        swap_token_mm : include/linux/swap.h:226
        Accessed at locs:
        mm/thrash.c:102 and
        include/linux/swap.h:244
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {swap_token_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: include/linux/swap.h:240
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/block/loop.c:832 w/ func: loop_thread

        made empty at: include/linux/swap.h:240
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: drivers/base/firmware_class.c:589 
        w/ func: request_firmware_work_func


dupe from 11_7_2007

===================================================
Possible race between access to:
        ((sem_ids.entries)->p[0])->sem_otime : ipc/sem.c:88 and
        ((sem_ids.entries)->p[0])->sem_otime : ipc/sem.c:88
        Accessed at locs:
        ipc/sem.c:1325 and
        ipc/sem.c:1325
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, } (1)
...

CHECK it

===================================================
Possible race between access to:
        ((socket->dev.kobj.kset)->dentry)->d_flags : drivers/pcmcia/cs.c:178 and
        ((socket->dev.dev)->kobj.dentry)->d_flags : drivers/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/dcache.h:180 and
        include/linux/dcache.h:180
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {((socket->dev.dev)->kobj.dentry)->d_lock#tbd, (((socket->dev.dev)->kobj.dentry
)->d_inode)->i_sem#tbd, } (2)
...



REASON: different lvals (seems possible that they're the same though)


===================================================
Possible race between access to:
        ((anon_vma_cachep->nodelists[0])->shared)->avail : include/linux/rmap.h:34 a
nd
        ((anon_vma_cachep->nodelists[0])->shared)->avail : include/linux/rmap.h:34
        Accessed at locs:
        mm/slab.c:2396 and
        mm/slab.c:2699
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: mm/rmap.c:117
        LS for 2nd access:
L+ = {(anon_vma_cachep->nodelists[0])->list_lock#tbd, } (1)
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: init/main.c:394 w/ func: init

(1)
        LS for 1st access:
L+ = empty;
        made empty at: mm/rmap.c:117
        LS for 2nd access:
L+ = {(anon_vma_cachep->nodelists[0])->list_lock#tbd, } (1)
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: init/do_mounts_initrd.c:59 w/ func: do_linuxrc

CHECK this

===================================================
Possible race between access to:
        ((per_cpu__radix_tree_preloads.nodes[0])->slots[0])->slots[0] : lib/radix-tr
ee.c:76 and
        ((per_cpu__radix_tree_preloads.nodes[0])->slots[0])->slots[0] : lib/radix-tr
ee.c:76
        Accessed at locs:
        lib/radix-tree.c:274 and
        lib/radix-tree.c:264
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {swapper_space.tree_lock#tbd, } (1)
        made empty at: lib/radix-tree.c:280
        Th. 1 spawned: drivers/base/firmware_class.c:589 w/ func: request_firmware_w
ork_func
        Th. 2 spawned: drivers/block/loop.c:832 w/ func: loop_thread

...

CHECK this


===================================================
Possible race between access to:
        (((GlobalOplock_Q.next)->tcon)->ses)->Suid : fs/cifs/cifsglob.h:483 and
        (((GlobalOplock_Q.next)->tcon)->ses)->Suid : fs/cifs/cifsglob.h:483
        Accessed at locs:
        fs/cifs/connect.c:2322 and
        fs/cifs/connect.c:2323
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(((GlobalOplock_Q.next)->tcon)->ses)->sesSem#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/cifs/cifssmb.c:112
        Th. 1 spawned: fs/cifs/cifsfs.c:980 w/ func: cifs_oplock_thread
        Th. 2 spawned: fs/cifs/cifsfs.c:980 w/ func: cifs_oplock_thread



CHECK this (another cifs thing?)


===================================================
Possible race between access to:
        (((dev_index_head[0].first)->ip_ptr)->mc_list)->sfcount[0] : net/core/dev.c:
178 and
        (((dev_index_head[0].first)->ip_ptr)->mc_list)->sfcount[0] : net/core/dev.c:
178
        Accessed at locs:
        net/ipv4/igmp.c:1562 and
        net/ipv4/igmp.c:2143
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(((dev_index_head[0].first)->ip_ptr)->mc_list)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: net/ipv4/igmp.c:1673
        Th. 1 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread
        Th. 2 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread


CHECK this?


===================================================
Possible race between access to:
(((_a137_408448_mempool->pages.next)->wb_context)->dentry)->d_inode : mm/mempool.c:115 and
(((_a137_408448_mempool->pages.next)->wb_context)->dentry)->d_inode : mm/mempool.c:115
        Accessed at locs:
        fs/dcache.c:101 and
        fs/nfs/write.c:1103
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(((_a137_408448_mempool->pages.next)->wb_context)->dentry)->d_lock#tbd, } (1)

        made empty at: fs/nfs/inode.c:1015
        Th. 1 spawned: fs/nfs/nfs4state.c:755 w/ func: reclaimer
        Th. 2 spawned: fs/nfs/nfs4state.c:755 w/ func: reclaimer



CHECK this (initialization?)



===================================================
Possible race between access to:
        nr_threads : include/linux/sched.h:95 and
        nr_threads : include/linux/sched.h:95
        Accessed at locs:
        kernel/exit.c:47 and
        kernel/fork.c:917
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {tasklist_lock#tbd, khpsbpkt_sig#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: kernel/fork.c:1148
        Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread
        Th. 2 spawned: net/ipv4/ipvs/ip_vs_sync.c:858 w/ func: fork_sync_thread

        etc...

CHECK this (similar to old report?)



===================================================
Possible race between access to:
        sync_recv_mesg_maxlen : net/ipv4/ipvs/ip_vs_sync.c:107 and
        ip_vs_sync_queue.next : net/ipv4/ipvs/ip_vs_sync.c:121
        Accessed at locs:
        net/ipv4/ipvs/ip_vs_sync.c:422 and
        include/linux/list.h:255
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {ip_vs_sync_lock#tbd, } (1)


REASON: different lvals (definite from memory layout)


===================================================
Possible race between access to:
(((_a918_394856_readinode.fragtree.rb_node)->node)->raw)->__totlen : fs/jffs
2/readinode.c:915 and
(((_a918_394856_readinode.fragtree.rb_node)->node)->raw)->__totlen : fs/jffs
2/readinode.c:915
        Accessed at locs:
        fs/jffs2/nodemgmt.c:695 and
        fs/jffs2/nodelist.h:241
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->erase_completion_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/nodemgmt.c:733
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


CHECK this (initialization?)


===================================================
Possible race between access to:
        ((GlobalOplock_Q.next)->tcon)->tidStatus : fs/cifs/cifsglob.h:483 and
        ((GlobalOplock_Q.next)->tcon)->tidStatus : fs/cifs/cifsglob.h:483
        Accessed at locs:
        fs/cifs/connect.c:3241 and
        fs/cifs/cifsfs.c:885
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(((GlobalOplock_Q.next)->tcon)->ses)->sesSem#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/cifs/cifssmb.c:112
        Th. 1 spawned: fs/cifs/cifsfs.c:980 w/ func: cifs_oplock_thread
        Th. 2 spawned: fs/cifs/cifsfs.c:980 w/ func: cifs_oplock_thread




CHECK this



===================================================
Possible race between access to:
        (fs_bio_set->bvec_pools[0])->curr_nr : fs/bio.c:76 and
        (fs_bio_set->bvec_pools[0])->curr_nr : fs/bio.c:76
        Accessed at locs:
        mm/mempool.c:27 and
        mm/mempool.c:225
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {page_pool->lock#tbd, isa_page_pool->lock#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: mm/mempool.c:242
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm

(1)
        LS for 1st access:
L+ = {(fs_bio_set->bio_pool)->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: mm/mempool.c:242
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm

(2)
        LS for 1st access:
L+ = {(fs_bio_set->bvec_pools[0])->lock#tbd, } (1)
L+ = empty;
        made empty at: mm/mempool.c:242
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm


REASON: ??? LS for first access is inconsistent, LS for second is empty



===================================================
Possible race between access to:
        rpc_mount : net/sunrpc/rpc_pipe.c:31 and
        rpc_mount : net/sunrpc/rpc_pipe.c:31
        Accessed at locs:
        fs/libfs.c:435 and
        fs/libfs.c:428
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {pin_fs_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: net/sunrpc/rpc_pipe.c:448
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: fs/nfs/nfs4state.c:755 w/ func: reclaimer

(1)
        LS for 1st access:
L+ = {pin_fs_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: net/sunrpc/rpc_pipe.c:448
        Th. 1 spawned: fs/lockd/clntlock.c:201 w/ func: reclaimer
        Th. 2 spawned: fs/nfs/delegation.c:319 w/ func: recall_thread


CHECK it (double-checked locking?)


===================================================
Possible race between access to:
        ip_vs_sync_queue.next : net/ipv4/ipvs/ip_vs_sync.c:121 and
        ip_vs_backup_syncid : net/ipv4/ipvs/ip_vs_sync.c:131
        Accessed at locs:
        include/linux/list.h:164 and
        net/ipv4/ipvs/ip_vs_sync.c:692
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {ip_vs_sync_lock#tbd, } (1)

REASON: different lvals (definite from memory layout)


===================================================
Possible race between access to:
        event : fs/namespace.c:43 and
        event : fs/namespace.c:43
        Accessed at locs:
        fs/namespace.c:137 and
        fs/namespace.c:144
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {vfsmount_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/pnode.c:219
        Th. 1 spawned: init/main.c:394 w/ func: init
        Th. 2 spawned: init/main.c:394 w/ func: init


CHECK it (double-checked locking?)


===================================================
Possible race between access to:
        sync_send_mesg_maxlen : net/ipv4/ipvs/ip_vs_sync.c:106 and
        ip_vs_sync_queue.next : net/ipv4/ipvs/ip_vs_sync.c:121
        Accessed at locs:
        net/ipv4/ipvs/ip_vs_sync.c:414 and
        include/linux/list.h:255
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;


REASON: different lvals (definite, just from struct memory layout)

===================================================
Possible race between access to:
        (c->erase_complete_list.next)->free_size : fs/jffs2/background.c:33 and
        (c->blocks)->free_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/erase.c:377 and
        fs/jffs2/debug.c:28
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->erase_free_sem#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/nodemgmt.c:441
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


REASON: possibly different lvals (another jffs2 gc thing)


===================================================
Possible race between access to:
        (c->blocks)->dirty_size : fs/jffs2/background.c:33 and
        (c->blocks)->dirty_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/nodemgmt.c:526 and
        fs/jffs2/nodemgmt.c:523
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->erase_completion_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/wbuf.c:402
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


CHECK this one



===================================================
Possible race between access to:
        socket->dev.node.next : drivers/pcmcia/cs.c:178 and
        (((socket->dev.kobj.kset)->dentry)->d_inode)->i_sb_list.next : drivers/pcmci
a/cs.c:178
        Accessed at locs:
        drivers/base/class.c:475 and
        include/linux/list.h:222
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: drivers/pcmcia/cs.c:699
        LS for 2nd access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.kset)->de
ntry)->d_inode)->i_sem#tbd, } (2)
        made empty at: fs/inode.c:266
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

REASON: different lvals (can tell from memory layout?)


----------
Proof ?

container_of(socket->dev.kobj.kset)->dentry->d_inode must point to 

(socket->dev.node - bits_offset(i_sb_list))


===================================================
Possible race between access to:
        ((tr->blkcore_priv)->rq)->unplug_timer.entry.next : drivers/mtd/mtd_blkdevs.
c:372 and
        ((tr->blkcore_priv)->rq)->queue_head.next : drivers/mtd/mtd_blkdevs.c:372
        Accessed at locs:
        include/linux/list.h:151 and
        include/linux/list.h:223
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(((tr->blkcore_priv)->rq)->unplug_timer.base)->lock#tbd, } (1)


REASON: different lvals (can prove just by reasoning on struct memory layout)



===================================================
Possible race between access to:
((journal->j_running_transaction)->t_reserved_list)->b_jlist : fs/jbd/journal.c:213 and
((journal->j_running_transaction)->t_locked_list)->b_jlist : fs/jbd/journal.c:213
        Accessed at locs:
        fs/jbd/transaction.c:1524 and
        fs/jbd/commit.c:408
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: fs/jbd/journal.c:215 w/ func: kjournald
        Th. 2 spawned: fs/jbd/journal.c:215 w/ func: kjournald

(1)
        LS for 1st access:
L+ = {journal->j_list_lock#tbd, journal->j_state_lock#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: fs/jbd/journal.c:215 w/ func: kjournald
        Th. 2 spawned: fs/jbd/journal.c:215 w/ func: kjournald


REASON: lvals different? (unable to prove via malloc analysis)



-----------
Proof?

check if 

*(blah.t_reserved_list) == *(blah.t_locked_list)



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
t_reserved_list & t_locked_list:



../linux-2.6.15/fs/jbd/transaction.c:           list = &transaction->t_reserved_list;
../linux-2.6.15/fs/jbd/transaction.c:           list = &transaction->t_reserved_list;


/* 
 * File a buffer on the given transaction list. 
 */
void __journal_file_buffer(struct journal_head *jh,
			transaction_t *transaction, int jlist)
{
	struct journal_head **list = NULL;
    ...
    
	assert_spin_locked(&transaction->t_journal->j_list_lock);

    ...

	switch (jlist) {

    ...

	case BJ_Reserved:
		list = &transaction->t_reserved_list;
		break;
	case BJ_Locked:
		list =  &transaction->t_locked_list;
		break;
	}

	__blist_add_buffer(list, jh);
	jh->b_jlist = jlist;

}



/*
 * Append a buffer to a transaction list, given the transaction's list head
 * pointer.
 *
 * j_list_lock is held.
 *
 * jbd_lock_bh_state(jh2bh(jh)) is held.
 */
static inline void 
__blist_add_buffer(struct journal_head **list, struct journal_head *jh)
{
	if (!*list) {
		jh->b_tnext = jh->b_tprev = jh;
		*list = jh;
	} else {
		/* Insert at the tail of the list to preserve order */
		struct journal_head *first = *list, *last = first->b_tprev;
		jh->b_tprev = last;
		jh->b_tnext = first;
		last->b_tnext = first->b_tprev = jh;
	}
}


// Looks like both lists get their targets from this same function
// and the target isn't a malloc'ed value (though the value) could
// be different on each call...




===================================================
Possible race between access to:
        cifs_req_poolp->curr_nr : fs/cifs/cifsfs.c:82 and
        cifs_req_poolp->curr_nr : fs/cifs/cifsfs.c:82
        Accessed at locs:
        mm/mempool.c:27 and
        mm/mempool.c:125
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {cifs_req_poolp->lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: mm/mempool.c:242
        Th. 1 spawned: fs/cifs/connect.c:1672 w/ func: cifs_demultiplex_thread
        Th. 2 spawned: fs/cifs/connect.c:1672 w/ func: cifs_demultiplex_thread


CHECK this (had a double-checked locking thing in mempool code before?)


===================================================
Possible race between access to:
        (rt_hash_table->chain)->u.rt_next : net/ipv4/route.c:238 and
        (rt_hash_table->chain)->u.rt_next : net/ipv4/route.c:238
        Accessed at locs:
        net/ipv4/route.c:934 and
        net/ipv4/route.c:926
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {rtnl_sem#tbd, (*rt_hash_locks)#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: net/ipv4/route.c:990
        Th. 1 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread
        Th. 2 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread


CHECK this (double-checked locking?)



===================================================
Possible race between access to:
        ((sem_ids.entries)->p[0])->sem_pending : ipc/sem.c:88 and
        ((sem_ids.entries)->p[0])->sem_pending : ipc/sem.c:88
        Accessed at locs:
        ipc/sem.c:272 and
        ipc/sem.c:272
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread
Th. 2 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_thread

(1)
        LS for 1st access:
L+ = {khpsbpkt_sig#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread
Th. 2 spawned: drivers/mmc/mmc_queue.c:161 w/ func: mmc_queue_thread

Th. 1 spawned: drivers/ieee1394/ieee1394_core.c:1071 w/ func: hpsbpkt_thread
Th. 2 spawned: drivers/media/video/tvaudio.c:1551 w/ func: chip_thread


CHECK this (some parent function didn't lock, or lval not shared?)



===================================================
Possible race between access to:
        mem_map->lru.prev : include/linux/mm.h:507 and
        mem_map->lru.prev : include/linux/mm.h:507
        Accessed at locs:
        mm/slab.c:584 and
        include/linux/list.h:163
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: kernel/signal.c:1042
        LS for 2nd access:
L+ = {(zone_table[0])->lru_lock#tbd, } (1)
        Th. 1 spawned: kernel/kmod.c:209 w/ func: ____call_usermodehelper
        Th. 2 spawned: kernel/kthread.c:112 w/ func: kthread

(1)
        LS for 1st access:
L+ = empty;
        made empty at: kernel/signal.c:1042
        LS for 2nd access:
L+ = {(zone_table[0])->lru_lock#tbd, } (1)
        Th. 1 spawned: kernel/kmod.c:209 w/ func: ____call_usermodehelper
        Th. 2 spawned: kernel/kmod.c:209 w/ func: ____call_usermodehelper


CHECK this


===================================================
Possible race between access to:
        ((GlobalOplock_Q.next)->tcon)->tid : fs/cifs/cifsglob.h:483 and
        ((GlobalOplock_Q.next)->tcon)->tid : fs/cifs/cifsglob.h:483
        Accessed at locs:
        fs/cifs/connect.c:3242 and
        fs/cifs/misc.c:316
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(((GlobalOplock_Q.next)->tcon)->ses)->sesSem#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: fs/cifs/cifsfs.c:980 w/ func: cifs_oplock_thread
        Th. 2 spawned: fs/cifs/cifsfs.c:980 w/ func: cifs_oplock_thread


CHECK this

hmm why don't these show up any more in the 3_8_2007 set? are these expansions of BLOBS?


===================================================
Possible race between access to:
(((tr->blkcore_priv)->rq)->flush_rq)->queuelist.prev : drivers/mtd/mtd_blkdevs.c:372 and
((tr->blkcore_priv)->rq)->unplug_timer.entry.prev : drivers/mtd/mtd_blkdevs.c:372
        Accessed at locs:
        block/ll_rw_blk.c:270 and
        kernel/timer.c:168
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {(((tr->blkcore_priv)->rq)->unplug_timer.base)->lock#tbd, } (1)
Th. 1 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_thread
Th. 2 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_thread



REASON: different lvals ("proved" below)



---------------
Proof?

int register_mtd_blktrans(struct mtd_blktrans_ops *tr)
{
	int ret, i;

    ...

	tr->blkcore_priv = kmalloc(sizeof(*tr->blkcore_priv), GFP_KERNEL);

    ...

	tr->blkcore_priv->rq = 
        blk_init_queue(mtd_blktrans_request, &tr->blkcore_priv->queue_lock);

    ^^^^ INIT (see below)

    ...

	tr->blkcore_priv->rq->queuedata = tr;

	ret = kernel_thread(mtd_blktrans_thread, tr, CLONE_KERNEL);

    ...
}

blkcore_priv is fresh, but how about blkcore_priv->rq, etc. ?



request_queue_t *blk_init_queue(request_fn_proc *rfn, spinlock_t *lock)
{
	return blk_init_queue_node(rfn, lock, -1);
}


request_queue_t *
blk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id)
{
	request_queue_t *q = blk_alloc_queue_node(GFP_KERNEL, node_id); <<<< HERE

    ...

    if (blk_init_free_list(q))
    
    ...

	q->request_fn		= rfn;
	q->back_merge_fn       	= ll_back_merge_fn;
	q->front_merge_fn      	= ll_front_merge_fn;
	q->merge_requests_fn	= ll_merge_requests_fn;

    ...

	blk_queue_make_request(q, __make_request); 

    ...
    
	/*
	 * all done
	 */
	if (!elevator_init(q, NULL)) {
		blk_queue_congestion_threshold(q);
		return q;
	}

	blk_cleanup_queue(q);
out_init:
	kmem_cache_free(requestq_cachep, q);
	return NULL;
}



~~~~~~~~~
flush_rq:

void blk_queue_ordered(request_queue_t *q, int flag)
{
	switch (flag) {
		case QUEUE_ORDERED_NONE:
			if (q->flush_rq)
				kmem_cache_free(request_cachep, q->flush_rq);
			q->flush_rq = NULL;
			q->ordered = flag;
			break;
		case QUEUE_ORDERED_TAG:
			q->ordered = flag;
			break;
		case QUEUE_ORDERED_FLUSH:
			q->ordered = flag;
			if (!q->flush_rq)
				q->flush_rq = kmem_cache_alloc(request_cachep,  <<<< FRESH
								GFP_KERNEL);
			break;
		default:
			printk("blk_queue_ordered: bad value %d\n", flag);
			break;
	}
}


since *flush_rq is fresh, *(flush_rq).queuelist is fresh


~~~~~~~~~~~~
unplug_timer

request_queue_t *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
{
	request_queue_t *q;

	q = kmem_cache_alloc_node(requestq_cachep, gfp_mask, node_id);
	if (!q)
		return NULL;

	memset(q, 0, sizeof(*q));

	init_timer(&q->unplug_timer);  <<<< HERE?

	atomic_set(&q->refcnt, 1);

	q->backing_dev_info.unplug_io_fn = blk_backing_dev_unplug;
	q->backing_dev_info.unplug_io_data = q;

	return q;
}

void fastcall init_timer(struct timer_list *timer)
{
	timer->entry.next = NULL;  <<<< FRESH

	timer->base = &per_cpu(tvec_bases, raw_smp_processor_id()).t_base;
}

since unplug_timer is a field of *q, and entry is part of unplug_timer, they
are fresh (wrt to flush_rq, which is also a field of *q) ...



===================================================
Possible race between access to:
        standbys_pending : arch/i386/kernel/apm.c:391 and
        apm_info.bios.flags : include/linux/apm_bios.h:109
        Accessed at locs:
        arch/i386/kernel/apm.c:1153 and
        arch/i386/kernel/apm.c:1057
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {user_list_lock#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm
        Th. 2 spawned: arch/i386/kernel/apm.c:2359 w/ func: apm



REASON: different lvals (definite from memory layout)

===================================================
Possible race between access to:
        cache_cache.colour : mm/slab.c:620 and
        cache_cache.colour : mm/slab.c:620
        Accessed at locs:
        mm/slab.c:1753 and
        mm/slab.c:2195
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {cache_cache.spinlock#tbd, } (1)
        Th. 1 spawned: init/main.c:394 w/ func: init
        Th. 2 spawned: init/main.c:394 w/ func: init


REASON: check it (happens during boot-time?)


===================================================
Possible race between access to:
        suspends_pending : arch/i386/kernel/apm.c:390 and
        standbys_pending : arch/i386/kernel/apm.c:391
        Accessed at locs:
        arch/i386/kernel/apm.c:1147 and
        arch/i386/kernel/apm.c:1413
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {user_list_lock#tbd, } (1)


REASON: different lvals (definite from memory layout)



===================================================
Possible race between access to:
        ip_vs_sync_queue.next : net/ipv4/ipvs/ip_vs_sync.c:121 and
        sync_master_pid : net/ipv4/ipvs/ip_vs_sync.c:622
        Accessed at locs:
        include/linux/list.h:164 and
        net/ipv4/ipvs/ip_vs_sync.c:756
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {ip_vs_sync_lock#tbd, } (1)


REASON: different lvals (definite from memory layout)


===================================================
Possible race between access to:
        c->erasing_size : fs/jffs2/background.c:33 and
        c->erasing_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/erase.c:133 and
        fs/jffs2/nodemgmt.c:754
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->erase_completion_lock#tbd, c->erase_free_sem#tbd, } (2)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/wbuf.c:402
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


--------
FIRST:

void jffs2_erase_pending_blocks(struct jffs2_sb_info *c, int count)
{
	struct jffs2_eraseblock *jeb;

	down(&c->erase_free_sem);

	spin_lock(&c->erase_completion_lock);

	while (!list_empty(&c->erase_complete_list) ||
	       !list_empty(&c->erase_pending_list)) {

		if (!list_empty(&c->erase_complete_list)) {

            ...
			spin_unlock(&c->erase_completion_lock);
			jffs2_mark_erased_block(c, jeb);
            ...

		} else if (!list_empty(&c->erase_pending_list)) {
			jeb = list_entry(c->erase_pending_list.next, struct jffs2_eraseblock, list);

			c->erasing_size += c->sector_size;  <<<< HERE!

            ...

			spin_unlock(&c->erase_completion_lock);

			jffs2_erase_block(c, jeb);

		} else {
			BUG();
		}

		/* Be nice */
		cond_resched();
		spin_lock(&c->erase_completion_lock);
	}

	spin_unlock(&c->erase_completion_lock);
	up(&c->erase_free_sem);
}


// has the locks we say it has

--------
SECOND:

int jffs2_thread_should_wake(struct jffs2_sb_info *c)
{
	...

	/* dirty_size contains blocks on erase_pending_list
	 * those blocks are counted in c->nr_erasing_blocks.
	 * If one block is actually erased, it is not longer counted as dirty_space
	 * but it is counted in c->nr_erasing_blocks, so we add it and subtract it
	 * with c->nr_erasing_blocks * c->sector_size again.
	 * Blocks on erasable_list are counted as dirty_size, but not in c->nr_erasing_blocks
	 * This helps us to force gc and pick eventually a clean block to spread the load.
	 */
	dirty = c->dirty_size + c->erasing_size - c->nr_erasing_blocks * c->sector_size;

	if (... (dirty > c->nospc_dirty_size))
		ret = 1;

    return 0;
}

called by:

void jffs2_garbage_collect_trigger(struct jffs2_sb_info *c)
{
	spin_lock(&c->erase_completion_lock);
        if (c->gc_task && jffs2_thread_should_wake(c)) <<<< HERE
                send_sig(SIGHUP, c->gc_task, 1);
	spin_unlock(&c->erase_completion_lock);
}

which has an overlapping lock

and:

static int jffs2_garbage_collect_thread(void *_c)
{
	struct jffs2_sb_info *c = _c;

	daemonize("jffs2_gcd_mtd%d", c->mtd->index);
	allow_signal(SIGKILL);
	allow_signal(SIGSTOP);
	allow_signal(SIGCONT);

	c->gc_task = current;
	complete(&c->gc_thread_start);

	set_user_nice(current, 10);

	for (;;) {
		allow_signal(SIGHUP);

		if (!jffs2_thread_should_wake(c)) {   <<<< HERE!

            ...
			schedule();
		}
    ...
}

// thread is fresh at this time, so there are no locks held


REASON: race, but not sure if it's harmful (no locks held when accessed at thread start, but interrupts enabled) 

could still race with "parent" thread?


===================================================
Possible race between access to:
(((socket->dev.kobj.kset)->dentry)->d_inode)->inotify_watches.next : drivers/pcmcia/cs.c:178 and
socket->thread_done.wait.task_list.next : drivers/pcmcia/cs.c:178
        Accessed at locs:
        include/linux/list.h:164 and
        kernel/sched.c:3240
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, ((socket->dev.kobj.dentry)->d
_inode)->inotify_sem#tbd, ((((socket->dev.kobj.dentry)->d_inode)->inotify_watches.ne
xt)->dev)->sem#tbd, } (3)

...

REASON: different lvals (likely, from memory layout)




===================================================
Possible race between access to:
(c->erase_complete_list.next)->wasted_size : fs/jffs2/background.c:33 and
(c->blocks)->wasted_size : fs/jffs2/background.c:33
        Accessed at locs:
        fs/jffs2/erase.c:380 and
        fs/jffs2/nodemgmt.c:523
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {c->erase_free_sem#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: fs/jffs2/wbuf.c:402
Th. 1 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread
Th. 2 spawned: fs/jffs2/background.c:44 w/ func: jffs2_garbage_collect_thread


REASON: possibly different lval (another jffs2 gc access)

===================================================
Possible race between access to:
        sync_backup_pid : net/ipv4/ipvs/ip_vs_sync.c:623 and
        ip_vs_sync_queue.next : net/ipv4/ipvs/ip_vs_sync.c:121
        Accessed at locs:
        net/ipv4/ipvs/ip_vs_sync.c:731 and
        include/linux/list.h:255
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = {ip_vs_sync_lock#tbd, } (1)
        Th. 1 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread
        Th. 2 spawned: net/ipv4/ipvs/ip_vs_sync.c:824 w/ func: sync_thread

REASON: different lvals (definite from memory layout)

===================================================
Possible race between access to:
        per_cpu__trickle_count : drivers/char/random.c:276 and
        per_cpu__trickle_count : drivers/char/random.c:276
        Accessed at locs:
        drivers/char/random.c:583 and
        drivers/char/random.c:583
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: :-1
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: net/bluetooth/hidp/core.c:634 w/ func: hidp_session
        Th. 2 spawned: net/bluetooth/hidp/core.c:634 w/ func: hidp_session

(1)
        LS for 1st access:
L+ = {(*(((tr->blkcore_priv)->rq)->queue_lock))#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        Th. 1 spawned: drivers/mtd/mtd_blkdevs.c:412 w/ func: mtd_blktrans_thread
        Th. 2 spawned: net/bluetooth/hidp/core.c:634 w/ func: hidp_session


REASON: per_cpu lval? (not all paths from thread roots hold locks either)

===================================================
Possible race between access to:
socket->dev.kobj.entry.prev : drivers/pcmcia/cs.c:178 and
(((socket->dev.kobj.kset)->dentry)->d_inode)->i_sb_list.prev : drivers/pcmcia/cs.c:178
        Accessed at locs:
        lib/kobject.c:126 and
        include/linux/list.h:222
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: drivers/base/class.c:587
        LS for 2nd access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.kset)->de
ntry)->d_inode)->i_sem#tbd, } (2)
        made empty at: fs/inode.c:266
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(1)
        LS for 1st access:
L+ = empty;
        made empty at: drivers/base/class.c:587
        LS for 2nd access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.kset)->de
ntry)->d_inode)->i_sem#tbd, } (2)
        made empty at: fs/fs-writeback.c:267
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd

(2)
        LS for 1st access:
L+ = empty;
        made empty at: drivers/base/class.c:587
        LS for 2nd access:
L+ = {((socket->dev.kobj.dentry)->d_inode)->i_sem#tbd, (((socket->dev.kobj.kset)->de
ntry)->d_inode)->i_sem#tbd, } (2)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd


REASON: possibly different lvals (and kobj doesn't have kset field)

by structure of memory, unlikely that (blah.d_inode) pts to something
compatible w/ (*socket) and offsets thereafter are incompatible




===================================================
Possible race between access to:
        socket->dev.node.next : drivers/pcmcia/cs.c:178 and
        ((socket->cb_dev)->subordinate)->devices.next : drivers/pcmcia/cs.c:178
        Accessed at locs:
        drivers/base/class.c:475 and
        drivers/pcmcia/cardbus.c:220
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: drivers/pcmcia/cs.c:699
        LS for 2nd access:
L+ = {socket->skt_sem#tbd, } (1)
        Th. 1 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd
        Th. 2 spawned: drivers/pcmcia/cs.c:243 w/ func: pccardd


REASON: possibly different lvals

------------------------------
Proof ? just need to show that

socket->cb_dev->subordinate doesn't point to:
(socket->dev.node - bits_offset(devices))






===================================================
Possible race between access to:
        (journal->j_running_transaction)->t_state : fs/jbd/journal.c:213 and
        (journal->j_checkpoint_transactions)->t_state : fs/jbd/journal.c:213
        Accessed at locs:
        fs/jbd/commit.c:215 and
        fs/jbd/checkpoint.c:623
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = empty;
        made empty at: fs/jbd/commit.c:794
        LS for 2nd access:
L+ = {journal->j_list_lock#tbd, journal->j_state_lock#tbd, } (2)
        Th. 1 spawned: fs/jbd/journal.c:215 w/ func: kjournald
        Th. 2 spawned: fs/jbd/journal.c:215 w/ func: kjournald


---------
FIRST: (recorded access has the j_state_lock), but another location

void journal_commit_transaction(journal_t *journal)
{
	transaction_t *commit_transaction;

    ...

	J_ASSERT(journal->j_running_transaction != NULL);
	J_ASSERT(journal->j_committing_transaction == NULL);

	commit_transaction = journal->j_running_transaction; <<<< 

	J_ASSERT(commit_transaction->t_state == T_RUNNING);  <<<< HERE!

    ...

	spin_lock(&journal->j_state_lock);

    ...
}

lock not held when checking J_ASSERT (not sure how they can fix this
w/out moving lock outside of the function call?)


----------
SECOND: holds the lock


----------

REASON: if same lval, it is a race, but looks like it can never be the same (first set to NULL before second set to first)


------------------
Proof?

see if 

*(journal->j_running_transaction ) == *(journal->j_checkpoint_transactions)

~~~~~~~~~~~~~~~~~~~~~~~
journal: (from fs/jbd/journal.c)

 
static void journal_start_thread(journal_t *journal)
{
	kernel_thread(kjournald, journal, CLONE_VM|CLONE_FS|CLONE_FILES);
	wait_event(journal->j_wait_done_commit, journal->j_task != 0);
}

seems more likely that they are handed off (not initialized to different 
fresh values) ?


~~~~~~~~~~~~~~~~~~~~~~~~
*.j_running_transaction:


../linux-2.6.15/fs/jbd/transaction.c:   journal->j_running_transaction = transaction;


static transaction_t *
get_transaction(journal_t *journal, transaction_t *transaction)
{
	transaction->t_journal = journal;
    ...
	J_ASSERT(journal->j_running_transaction == NULL);
	journal->j_running_transaction = transaction;

	return transaction;
}

called by:

static int start_this_handle(journal_t *journal, handle_t *handle)
{
	transaction_t *transaction;
	transaction_t *new_transaction = NULL;

    ...

alloc_transaction:
	if (!journal->j_running_transaction) {
		new_transaction = jbd_kmalloc(sizeof(*new_transaction),
						GFP_NOFS);
		if (!new_transaction) {
			ret = -ENOMEM;
			goto out;
		}
		memset(new_transaction, 0, sizeof(*new_transaction));
	}

    // else, it uses the old one

repeat:

	/*
	 * We need to hold j_state_lock until t_updates has been incremented,
	 * for proper journal barrier handling
	 */
	spin_lock(&journal->j_state_lock);
repeat_locked:

    ...

	if (!journal->j_running_transaction) {
		if (!new_transaction) {
			spin_unlock(&journal->j_state_lock);
			goto alloc_transaction;
		}
		get_transaction(journal, new_transaction);   <<<< SET HERE
		new_transaction = NULL;
	}

    ... runs and stuff?

}

// still fresh if journal didn't already have a running transaction, but
// in what case does a journal already have a running transaction?
// It can only get the transaction by going through the same functions... 
// (transaction has one fresh source, but what happens in between?)




~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*.j_checkpoint_transactions:



../linux-2.6.15/fs/jbd/checkpoint.c:                    journal->j_checkpoint_transactions =
../linux-2.6.15/fs/jbd/checkpoint.c:                    journal->j_checkpoint_transactions = NULL;

/*
 * We've finished with this transaction structure: adios...
 * 
 * The transaction must have no links except for the checkpoint by this
 * point.
 *
 * Called with the journal locked.
 * Called with j_list_lock held.
 */

void __journal_drop_transaction(journal_t *journal, transaction_t *transaction)
{
	assert_spin_locked(&journal->j_list_lock);

	if (transaction->t_cpnext) {
		transaction->t_cpnext->t_cpprev = transaction->t_cpprev;
		transaction->t_cpprev->t_cpnext = transaction->t_cpnext;

		if (journal->j_checkpoint_transactions == transaction)
			journal->j_checkpoint_transactions =
				transaction->t_cpnext;

		if (journal->j_checkpoint_transactions == transaction)
			journal->j_checkpoint_transactions = NULL;

	}

    ...

	J_ASSERT(journal->j_committing_transaction != transaction);
	J_ASSERT(journal->j_running_transaction != transaction);

	kfree(transaction);
}

j_running_transaction != transaction, but may still equal transaction->t_cpnext


--------------------


../linux-2.6.15/fs/jbd/commit.c:                        journal->j_checkpoint_transactions = commit_transaction;


void journal_commit_transaction(journal_t *journal)
{
	transaction_t *commit_transaction;

    ...

	commit_transaction = journal->j_running_transaction;  <<<< START

	J_ASSERT(commit_transaction->t_state == T_RUNNING);

	spin_lock(&journal->j_state_lock);
	commit_transaction->t_state = T_LOCKED;

    ...

	commit_transaction->t_state = T_FLUSH;
	journal->j_committing_transaction = commit_transaction;

	journal->j_running_transaction = NULL;                <<<< running killed

	commit_transaction->t_log_start = journal->j_head;
	wake_up(&journal->j_wait_transaction_locked);
	spin_unlock(&journal->j_state_lock);


    ...

	spin_lock(&journal->j_state_lock);

    ...

	commit_transaction->t_state = T_FINISHED;
	J_ASSERT(commit_transaction == journal->j_committing_transaction);
    ...

	spin_unlock(&journal->j_state_lock);

	if (commit_transaction->t_checkpoint_list == NULL) {
		__journal_drop_transaction(journal, commit_transaction);
	} else {
		if (journal->j_checkpoint_transactions == NULL) {

			journal->j_checkpoint_transactions = commit_transaction;  <<< END

			commit_transaction->t_cpnext = commit_transaction;
			commit_transaction->t_cpprev = commit_transaction;
		} else {
			commit_transaction->t_cpnext =
				journal->j_checkpoint_transactions;
			commit_transaction->t_cpprev =
				commit_transaction->t_cpnext->t_cpprev;
			commit_transaction->t_cpnext->t_cpprev =
				commit_transaction;
			commit_transaction->t_cpprev->t_cpnext =
				commit_transaction;
		}
	}
    
    ...

}

// Flow between the two in this function, but j_running_transaction is set
// to NULL before j_checkpoint_transactions is set to commit_transaction





===================================================
Possible race between access to:
        pktgen_threads : net/core/pktgen.c:495 and
        debug : net/core/pktgen.c:492
        Accessed at locs:
        net/core/pktgen.c:2521 and
        net/core/pktgen.c:2617
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {pktgen_sem#tbd, } (1)
        LS for 2nd access:
L+ = empty;
        made empty at: :-1
        Th. 1 spawned: net/core/pktgen.c:2942 w/ func: pktgen_thread_worker
        Th. 2 spawned: net/core/pktgen.c:2942 w/ func: pktgen_thread_worker



REASON: different lvals (definite from memory layout)




===================================================
Possible race between access to:
per_cpu__softnet_data.input_pkt_queue.qlen : include/linux/netdevice.h:593 and
per_cpu__softnet_data.input_pkt_queue.qlen : include/linux/netdevice.h:593
        Accessed at locs:
        include/linux/skbuff.h:664 and
        include/linux/skbuff.h:664
        Possible paths & LS (first 3):

(0)
        LS for 1st access:
L+ = {(session_list.next)->tx_queue.lock#tbd, } (1)


REASON: lval is named "per_cpu"... and the lock seems unrelated to the lval



from Anh:

concinnity \kuhn-SIN-uh-tee\, noun:
1. Internal harmony or fitness in the adaptation of parts to a whole or 
    to each other.
